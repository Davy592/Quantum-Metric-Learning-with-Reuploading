{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.533159Z",
     "start_time": "2025-02-25T12:56:28.526193Z"
    }
   },
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "import sklearn.metrics.cluster as cluster_metrics\n",
    "import numpy as np\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
    "from sklearn.manifold import TSNE\n",
    "from qiskit.circuit.library import RealAmplitudes, TwoLocal\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import torch\n",
    "from torch import nn\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.583214Z",
     "start_time": "2025-02-25T12:56:28.573346Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set all needed seed\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    Args:\n",
    "        seed (int): Seed value\n",
    "    \"\"\"\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # CUDA deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Qiskit determinism\n",
    "    algorithm_globals.random_seed = seed\n",
    "    \n",
    "    # Environment variables\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.663103Z",
     "start_time": "2025-02-25T12:56:28.658970Z"
    }
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "print(\"CUDA Available:  \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using {color.BOLD} {str(device).upper()} {color.END} Acceleration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.687986Z",
     "start_time": "2025-02-25T12:56:28.685260Z"
    }
   },
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    contingency_matrix = cluster_metrics.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "def evaluate_clustering(X, pred_y, true_y):\n",
    "    print(f\"{'Silhouette:':15s}{silhouette_score(X, pred_y):2.3f}\")\n",
    "    print(f\"{'Purity:':15s}{purity_score(true_y, pred_y):2.3f}\")\n",
    "\n",
    "def evaluate_clustering_table(X, pred_y, true_y):\n",
    "    return silhouette_score(X, pred_y), purity_score(true_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.788896Z",
     "start_time": "2025-02-25T12:56:28.713256Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# DATASET CONFIGURATION\n",
    "# ===========================\n",
    "# Scegli il dataset: \"MNIST\" o \"FashionMNIST\"\n",
    "DATASET_TYPE = \"MNIST\"  # Cambia in \"MNIST\" per usare MNIST\n",
    "\n",
    "INITIAL_TRAINING_SAMPLES = 400\n",
    "\n",
    "\n",
    "MNIST_base_transform = transforms.Compose([\n",
    "transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Caricamento dataset in base alla scelta\n",
    "if DATASET_TYPE == \"MNIST\":\n",
    "    MNIST_dataset_train = datasets.MNIST(root=\"quantum_metric_learning-master/data/dataset/MNIST\", train=True, download=True, transform=MNIST_base_transform)\n",
    "    MNIST_dataset_test = datasets.MNIST(root=\"quantum_metric_learning-master/data/dataset/MNIST\", train=False, download=True, transform=MNIST_base_transform)\n",
    "    print(f\"{color.BOLD}Using MNIST dataset{color.END}\")\n",
    "elif DATASET_TYPE == \"FashionMNIST\":\n",
    "    MNIST_dataset_train = datasets.FashionMNIST(root=\"quantum_metric_learning-master/data/dataset/FashionMNIST\", train=True, download=True, transform=MNIST_base_transform)\n",
    "    MNIST_dataset_test = datasets.FashionMNIST(root=\"quantum_metric_learning-master/data/dataset/FashionMNIST\", train=False, download=True, transform=MNIST_base_transform)\n",
    "    print(f\"{color.BOLD}Using Fashion MNIST dataset{color.END}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset type: {DATASET_TYPE}. Choose 'MNIST' or 'FashionMNIST'\")\n",
    "\n",
    "\n",
    "if False:\n",
    "    index01 = MNIST_dataset_train.train_labels <= 1\n",
    "    MNIST_dataset_train.data = MNIST_dataset_train.data[index01]\n",
    "    MNIST_dataset_train.targets = MNIST_dataset_train.targets[index01]\n",
    "\n",
    "    index01_test = MNIST_dataset_test.train_labels <= 1\n",
    "    MNIST_dataset_test.data = MNIST_dataset_test.data[index01_test]\n",
    "    MNIST_dataset_test.targets = MNIST_dataset_test.targets[index01_test]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"FULL DATASET INFO\")\n",
    "print(f\"Image shape            : {MNIST_dataset_train.data.shape}\")\n",
    "print(f\"Total training samples : {len(MNIST_dataset_train)}\")\n",
    "print(f\"Total test     samples : {len(MNIST_dataset_test)}\")\n",
    "print(\"\")\n",
    "\n",
    "init_train_data, _, init_train_target, _ = train_test_split(\n",
    "    range(len(MNIST_dataset_train)), \n",
    "    MNIST_dataset_train.targets,\n",
    "    random_state=42,\n",
    "    stratify=MNIST_dataset_train.targets,\n",
    "    test_size=len(MNIST_dataset_train)- INITIAL_TRAINING_SAMPLES)\n",
    "\n",
    "\n",
    "X = MNIST_dataset_train.data[init_train_data].numpy().astype(\"float32\") / 255\n",
    "y = MNIST_dataset_train.targets[init_train_data].numpy().astype(\"float32\") \n",
    "\n",
    "print(\"TRAINING DATA INFO\")\n",
    "print(f\"Image shape            : {X.shape}\")\n",
    "print(f\"Total training samples : {len(X)}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "'''\n",
    "init_test_data, _, init_test_target, _ = train_test_split(\n",
    "    range(len(MNIST_dataset_test)), \n",
    "    MNIST_dataset_test.targets,\n",
    "    random_state=42,\n",
    "    stratify=MNIST_dataset_test.targets,\n",
    "    test_size=len(MNIST_dataset_test)- INITIAL_TESTING_SAMPLES)\n",
    "'''\n",
    "\n",
    "X_test = MNIST_dataset_test.data.numpy().astype(\"float32\") / 255\n",
    "y_test = MNIST_dataset_test.targets.numpy().astype(\"float32\")\n",
    "\n",
    "print(\"TESTING DATA INFO\")\n",
    "print(f\"Image shape            : {X_test.shape}\")\n",
    "print(f\"Total training samples : {len(X_test)}\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.817718Z",
     "start_time": "2025-02-25T12:56:28.813326Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNIST_Distance_Dataset_Triplet_Loss(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.kers = np.ones((self.data.shape[0], self.data.shape[0]))\n",
    "        self.output_transform = transforms.ToTensor()\n",
    "        self.len = len(self.target)\n",
    "        self.classes = {i:(np.where(self.target == i)[0], np.where(self.target != i)[0] ) for i in range(10)}\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_anchor(idx), self.get_positive(idx), self.get_negative(idx)\n",
    "    \n",
    "    def get_anchor(self, idx):\n",
    "        return self.output_transform(self.data[idx])\n",
    "    \n",
    "\n",
    "    def get_positive(self, idx):\n",
    "        i = np.random.choice(self.classes[self.target[idx]][0])\n",
    "        return self.output_transform(self.data[i])\n",
    "    \n",
    "    def get_negative(self, idx):\n",
    "        i = np.random.choice(self.classes[self.target[idx]][1])\n",
    "        return self.output_transform(self.data[i])\n",
    "\n",
    "\n",
    "    def get_order(self):\n",
    "        return self.target.argsort()\n",
    "\n",
    "\n",
    "    def ordered_pairwise(self):\n",
    "        return self.kers[:,self.get_order()][self.get_order(),:]\n",
    "\n",
    "\n",
    "    def get_flatten(self):\n",
    "        return self.data.reshape((self.data.shape[0], self.data.shape[1]**2))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.973983Z",
     "start_time": "2025-02-25T12:56:28.842175Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "t = MNIST_Distance_Dataset_Triplet_Loss(X_train, y_train)\n",
    "v = MNIST_Distance_Dataset_Triplet_Loss(X_val, y_val)\n",
    "#stampa lunghezza del dataset\n",
    "\n",
    "print(len(t))\n",
    "print(len(v))\n",
    "\n",
    "anchor, pos, neg = t[0]\n",
    "\n",
    "\n",
    "plt.matshow(anchor.squeeze(0))\n",
    "plt.matshow(pos.squeeze(0))\n",
    "plt.matshow(neg.squeeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:29.874119Z",
     "start_time": "2025-02-25T12:56:29.004013Z"
    }
   },
   "outputs": [],
   "source": [
    "reduction_model = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "\n",
    "vis_x = reduction_model.fit_transform(t.get_flatten(), t.target)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Data visualization using TSNE\", weight=\"bold\")\n",
    "ax.scatter(vis_x[:,0], vis_x[:,1], c=t.target, cmap='Dark2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Amplitude encoding, richiede nlog2 features\n",
    "'''\n",
    "def amplitude_encoding(n_features, param_name):\n",
    "  qc = RawFeatureVector(n_features)\n",
    "  qc = qc.assign_parameters(ParameterVector(param_name, n_features))\n",
    "  qc.name = f\"Amplitude Encoding {param_name}\"\n",
    "  return qc\n",
    "\n",
    "amplitude_encoding(4, \"a\").draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YZ angles encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yz_angles_encoding(n_features, param_name=\"x\", params=None):\n",
    "  \"\"\"\n",
    "  Encoding YZ con rotazioni RY e RZ.\n",
    "  \n",
    "  Args:\n",
    "      n_features: numero di feature\n",
    "      param_name: nome dei parametri (crea un nuovo ParameterVector)\n",
    "      params: ParameterVector esistente da riutilizzare\n",
    "  \n",
    "  Nota: Specificare uno solo tra param_name e params\n",
    "  \"\"\"\n",
    "  if params is None:\n",
    "    params = ParameterVector(param_name, n_features)\n",
    "  \n",
    "  n_qubit = math.ceil(n_features / 2)\n",
    "  qc = QuantumCircuit(n_qubit, name=f\"Angles Encoding\")\n",
    "  gates = [qc.ry, qc.rz]\n",
    "\n",
    "  for i in range(n_qubit):\n",
    "    for gate_i in range(2):\n",
    "      pindex = i*2 + gate_i\n",
    "      if pindex < n_features:\n",
    "        gates[gate_i](params[pindex], i)\n",
    "\n",
    "  return qc\n",
    "\n",
    "yz_angles_encoding(8, param_name=\"x\").draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:29.992265Z",
     "start_time": "2025-02-25T12:56:29.919089Z"
    }
   },
   "outputs": [],
   "source": [
    "def pooling_layer(in_lane, param_prefix=\"pool\"):\n",
    "  qc = QuantumCircuit(in_lane, name=\"Pooling Layer\")\n",
    "  params = ParameterVector(param_prefix, length=in_lane //2 *3)\n",
    "\n",
    "  for i in range(in_lane//2):\n",
    "    current = i\n",
    "    aux = i+ in_lane//2\n",
    "\n",
    "    base_param =  current*(in_lane//2 -1)\n",
    "\n",
    "    qc.rz(-np.pi/2, aux)\n",
    "    qc.cx(aux, current)\n",
    "    qc.rz(params[base_param + 0], current)\n",
    "    qc.ry(params[base_param + 1], aux)\n",
    "    qc.cx(current, aux)\n",
    "    qc.ry(params[base_param + 2], aux)\n",
    "\n",
    "  return qc\n",
    "\n",
    "pooling_layer(4, param_prefix=\"m\").draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFICHE ENCODING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HRyRx encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:30.053492Z",
     "start_time": "2025-02-25T12:56:30.006800Z"
    }
   },
   "outputs": [],
   "source": [
    "#Modifiche PROF\n",
    "def hRyRx_encoding(n_features):\n",
    "  # qubits\n",
    "  n_qubits = math.floor(n_features /2) + (1 if (n_features % 2) != 0 else 0)\n",
    "  # feature extracted from neural network\n",
    "  n_feature = n_features\n",
    "  feature_map = QuantumCircuit(n_qubits)\n",
    "  input_params = ParameterVector(name='x', length=n_feature)\n",
    "  idx = 0\n",
    "  for i in range(n_qubits):\n",
    "      feature_map.h(i)\n",
    "      feature_map.ry(input_params[idx], i)\n",
    "      feature_map.rx(input_params[idx+1], i)\n",
    "      idx +=2\n",
    "\n",
    "  return feature_map\n",
    "\n",
    "hRyRx_encoding(8).draw(output=\"mpl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HRyRz encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:30.119520Z",
     "start_time": "2025-02-25T12:56:30.076547Z"
    }
   },
   "outputs": [],
   "source": [
    "def hRyRz_encoding(n_features):\n",
    "  # qubits\n",
    "  n_qubits = math.floor(n_features /2) + (1 if (n_features % 2) != 0 else 0)\n",
    "  # feature extracted from neural network\n",
    "  n_feature = n_features\n",
    "  feature_map = QuantumCircuit(n_qubits)\n",
    "  input_params = ParameterVector(name='x', length=n_feature)\n",
    "  idx = 0\n",
    "  for i in range(n_qubits):\n",
    "      feature_map.h(i)\n",
    "      feature_map.ry(input_params[idx], i)\n",
    "      feature_map.rz(input_params[idx+1], i)\n",
    "      idx +=2\n",
    "  return feature_map\n",
    "\n",
    "hRyRz_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X angles encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:30.274547Z",
     "start_time": "2025-02-25T12:56:30.244645Z"
    }
   },
   "outputs": [],
   "source": [
    "def x_angles_encoding(n_features, param_name):\n",
    "  params = ParameterVector(param_name, n_features)\n",
    "  n_qubit = n_features\n",
    "  qc = QuantumCircuit(n_qubit, name=f\"Angles Encoding {param_name}\")\n",
    "  \n",
    "\n",
    "  for i in range(n_qubit):\n",
    "    qc.rx(params[i], i)\n",
    "\n",
    "  return qc\n",
    "\n",
    "x_angles_encoding(4, \"a\").draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RxRy encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RxRy_encoding(n_features, params=None):\n",
    "  \"\"\"\n",
    "  Encoding RxRy con rotazioni RX e RY.\n",
    "  \n",
    "  Args:\n",
    "      n_features: numero di feature\n",
    "      params: ParameterVector da utilizzare (se None, ne crea uno nuovo con nome 'x')\n",
    "  \"\"\"\n",
    "  if params is None:\n",
    "      params = ParameterVector(name='x', length=n_features)\n",
    "  \n",
    "  n_qubits = math.ceil(n_features / 2)\n",
    "  feature_map = QuantumCircuit(n_qubits)\n",
    "  idx = 0\n",
    "  \n",
    "  for i in range(n_qubits):\n",
    "      if idx < n_features:\n",
    "          feature_map.rx(params[idx], i)\n",
    "      if idx + 1 < n_features:\n",
    "          feature_map.ry(params[idx + 1], i)\n",
    "      idx += 2\n",
    "  \n",
    "  return feature_map\n",
    "\n",
    "RxRy_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RxRz encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RxRz_encoding(n_features, params=None):\n",
    "  \"\"\"\n",
    "  Encoding RxRz con rotazioni RX e RZ.\n",
    "  \n",
    "  Args:\n",
    "      n_features: numero di feature\n",
    "      params: ParameterVector da utilizzare (se None, ne crea uno nuovo con nome 'x')\n",
    "  \"\"\"\n",
    "  if params is None:\n",
    "      params = ParameterVector(name='x', length=n_features)\n",
    "  \n",
    "  n_qubits = math.ceil(n_features / 2)\n",
    "  feature_map = QuantumCircuit(n_qubits)\n",
    "  idx = 0\n",
    "  \n",
    "  for i in range(n_qubits):\n",
    "      if idx < n_features:\n",
    "          feature_map.rx(params[idx], i)\n",
    "      if idx + 1 < n_features:\n",
    "          feature_map.rz(params[idx + 1], i)\n",
    "      idx += 2\n",
    "  \n",
    "  return feature_map\n",
    "\n",
    "RxRz_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RyRx encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RyRx_encoding(n_features, params=None):\n",
    "  \"\"\"\n",
    "  Encoding RyRx con rotazioni RY e RX.\n",
    "  \n",
    "  Args:\n",
    "      n_features: numero di feature\n",
    "      params: ParameterVector da utilizzare (se None, ne crea uno nuovo con nome 'x')\n",
    "  \"\"\"\n",
    "  if params is None:\n",
    "      params = ParameterVector(name='x', length=n_features)\n",
    "  \n",
    "  n_qubits = math.ceil(n_features / 2)\n",
    "  feature_map = QuantumCircuit(n_qubits)\n",
    "  idx = 0\n",
    "  \n",
    "  for i in range(n_qubits):\n",
    "      if idx < n_features:\n",
    "          feature_map.ry(params[idx], i)\n",
    "      if idx + 1 < n_features:\n",
    "          feature_map.rx(params[idx + 1], i)\n",
    "      idx += 2\n",
    "  \n",
    "  return feature_map\n",
    "\n",
    "RyRx_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RzRx encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RzRx_encoding(n_features, params=None):\n",
    "  \"\"\"\n",
    "  Encoding RzRx con rotazioni RZ e RX.\n",
    "  \n",
    "  Args:\n",
    "      n_features: numero di feature\n",
    "      params: ParameterVector da utilizzare (se None, ne crea uno nuovo con nome 'x')\n",
    "  \"\"\"\n",
    "  if params is None:\n",
    "      params = ParameterVector(name='x', length=n_features)\n",
    "  \n",
    "  n_qubits = math.ceil(n_features / 2)\n",
    "  feature_map = QuantumCircuit(n_qubits)\n",
    "  idx = 0\n",
    "  \n",
    "  for i in range(n_qubits):\n",
    "      if idx < n_features:\n",
    "          feature_map.rz(params[idx], i)\n",
    "      if idx + 1 < n_features:\n",
    "          feature_map.rx(params[idx + 1], i)\n",
    "      idx += 2\n",
    "  \n",
    "  return feature_map\n",
    "\n",
    "RzRx_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear full entanglement encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map_nonlinear_full_entaglement(num_features, params):\n",
    "    \"\"\"Feature map con non linearità (quadratiche) e full entaglement\"\"\"\n",
    "    qc = QuantumCircuit(num_features)\n",
    "    for i in range(num_features):\n",
    "        qc.ry(params[i] ** 2, i)\n",
    "    for i in range(num_features):\n",
    "        for j in range(i + 1, num_features):\n",
    "            qc.cz(i, j)\n",
    "    return qc\n",
    "\n",
    "feature_map_nonlinear_full_entaglement(4, [0.1, 0.2, 0.3, 0.4]).draw(output='mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Reuploading Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_reuploading_encoding(n_features, param_name=\"x\", layers=3):\n",
    "    \"\"\"\n",
    "    Standard Data Reuploading: i dati vengono caricati più volte nel circuito\n",
    "    in layer successivi con encoding diversi e entanglement.\n",
    "    Solo encoding e CNOT, nessun ansatz.\n",
    "    \n",
    "    Struttura (repeating cycle):\n",
    "    - Layer 0, 3, 6, ...: yz_angles_encoding (RY, RZ)\n",
    "    - Layer 1, 4, 7, ...: RxRy_encoding (RX, RY)\n",
    "    - Layer 2, 5, 8, ...: RzRx_encoding (RZ, RX)\n",
    "    \n",
    "    Args:\n",
    "        n_features: numero di feature in input\n",
    "        param_name: nome dei parametri\n",
    "        layers: numero di layer di reuploading (default=3)\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit con standard reuploading\n",
    "    \"\"\"\n",
    "    n_qubits = math.ceil(n_features / 2)\n",
    "    qc = QuantumCircuit(n_qubits, name=f\"Standard Reuploading\")\n",
    "    params = ParameterVector(param_name, n_features)\n",
    "    \n",
    "    for layer in range(layers):\n",
    "        # Encoder sequence repeats cyclically\n",
    "        encoder_choice = layer % 3\n",
    "        \n",
    "        if encoder_choice == 0:\n",
    "            # YZ encoding (RY, RZ)\n",
    "            encoder_block = yz_angles_encoding(n_features, params=params)\n",
    "                \n",
    "        elif encoder_choice == 1:\n",
    "            # RX-RY encoding (RX, RY)\n",
    "            encoder_block = RxRy_encoding(n_features, params=params)\n",
    "                \n",
    "        else:\n",
    "            # RZ-RX encoding (RZ, RX)\n",
    "            encoder_block = RzRx_encoding(n_features, params=params)\n",
    "\n",
    "        qc.compose(encoder_block, inplace=True)\n",
    "        qc.barrier()\n",
    "        \n",
    "        # Entanglement layer con CNOT\n",
    "        for i in range(n_qubits - 1):\n",
    "            qc.cx(i, i + 1)\n",
    "        \n",
    "        # Entanglement circolare\n",
    "        if n_qubits > 2:\n",
    "            qc.cx(n_qubits - 1, 0)\n",
    "        \n",
    "        qc.barrier()\n",
    "        qc.barrier()\n",
    "    \n",
    "    return qc\n",
    "\n",
    "# Test del circuito\n",
    "standard_reuploading_encoding(8, param_name=\"x\", layers=2).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Reuploading Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_reuploading_encoding(n_features, layers=3, features_per_layer=4):\n",
    "    \"\"\"\n",
    "    Partial Data Reuploading con finestra scorrevole sequenziale e wrapping ciclico.\n",
    "    Ogni layer carica feature DIVERSE su qubit sovrapposti in modo sequenziale.\n",
    "    Il ciclo di feature è determinato da layers * features_per_layer e ricomincia.\n",
    "    Solo encoding (RY, RZ) ed entanglement (CNOT), nessun ansatz.\n",
    "    \n",
    "    Esempio con 3 layer, 4 feature/layer:\n",
    "    - Layer 0: feature 0-3  → qubit (0,1)\n",
    "    - Layer 1: feature 4-7  → qubit (1,2)  \n",
    "    - Layer 2: feature 0-3  → qubit (2,3)  [restarts from beginning]\n",
    "    Ciclo totale: 8 feature, indipendentemente da n_features\n",
    "    \n",
    "    Args:\n",
    "        n_features: numero TOTALE di feature disponibili (deve essere >= features_per_layer)\n",
    "        layers: numero di blocchi di encoding\n",
    "        features_per_layer: quante feature caricare in ogni layer (deve essere pari)\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit con partial reuploading sequenziale con wrapping\n",
    "    \"\"\"\n",
    "    # Assicurati che le feature per layer siano un numero pari\n",
    "    if features_per_layer % 2 != 0:\n",
    "        features_per_layer += 1\n",
    "        print(f\"Adattato features_per_layer a {features_per_layer} per l'encoding a coppie.\")\n",
    "    \n",
    "    # Calcolo del ciclo totale di feature (indipendente da n_features)\n",
    "    total_cycle = layers * features_per_layer\n",
    "    \n",
    "    # Assicurati che n_features sia sufficiente per una iterazione del ciclo\n",
    "    if n_features < features_per_layer:\n",
    "        raise ValueError(f\"n_features ({n_features}) deve essere >= features_per_layer ({features_per_layer})\")\n",
    "    \n",
    "    # Calcolo del numero di qubit necessari per la finestra scorrevole\n",
    "    n_qubits_per_encoding = math.ceil(features_per_layer / 2)\n",
    "    n_qubits = n_qubits_per_encoding + (layers - 1)\n",
    "    \n",
    "    # Inizializziamo il circuito con ciclo indipendente da n_features\n",
    "    qc = QuantumCircuit(n_qubits, name=\"Partial Reuploading Sequential\")\n",
    "    input_params = ParameterVector(\"x\", total_cycle)\n",
    "    \n",
    "    for layer in range(layers):\n",
    "        # Calcola l'offset dei qubit per questo layer (finestra scorrevole)\n",
    "        qubit_offset = layer\n",
    "        \n",
    "        # Calcola l'indice di partenza delle feature per questo layer (cicla su total_cycle)\n",
    "        start_feature_idx = (layer * features_per_layer) % total_cycle\n",
    "        \n",
    "        # Carica il blocco di feature sui qubit della finestra corrente con wrapping ciclico\n",
    "        feature_idx = 0\n",
    "        for i in range(n_qubits_per_encoding):\n",
    "            qubit_target = qubit_offset + i\n",
    "            \n",
    "            # Calcola gli indici delle feature con wrapping ciclico su total_cycle\n",
    "            p_idx_1 = (start_feature_idx + feature_idx) % total_cycle\n",
    "            p_idx_2 = (start_feature_idx + feature_idx + 1) % total_cycle\n",
    "            \n",
    "            # Applica i gate di encoding\n",
    "            qc.ry(input_params[p_idx_1], qubit_target)\n",
    "            qc.rz(input_params[p_idx_2], qubit_target)\n",
    "            \n",
    "            feature_idx += 2\n",
    "\n",
    "        qc.barrier()\n",
    "        \n",
    "        # Entanglement layer sui qubit di questo layer\n",
    "        qubits_in_layer = list(range(qubit_offset, qubit_offset + n_qubits_per_encoding))\n",
    "        for i in range(len(qubits_in_layer) - 1):\n",
    "            qc.cx(qubits_in_layer[i], qubits_in_layer[i + 1])\n",
    "        \n",
    "        # Entanglement circolare (opzionale, solo se ci sono più di 2 qubit)\n",
    "        if len(qubits_in_layer) > 2:\n",
    "            qc.cx(qubits_in_layer[-1], qubits_in_layer[0])\n",
    "        \n",
    "        qc.barrier()\n",
    "        qc.barrier()\n",
    "    \n",
    "    return qc\n",
    "\n",
    "# Test del circuito - ciclo di 8 feature (2 layer * 4 feature/layer), stesso con n_features=24, 12, 8, ecc.\n",
    "partial_reuploading_encoding(n_features=12, layers=2, features_per_layer=4).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSATZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:30.360308Z",
     "start_time": "2025-02-25T12:56:30.357268Z"
    }
   },
   "outputs": [],
   "source": [
    "def MPS(num_qubits,parameter_prefix=\"x\", **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a Matrix Product State (MPS) quantum circuit.\n",
    "\n",
    "    Args:\n",
    "        num_qubits (int): The number of qubits in the circuit.\n",
    "        **kwargs: Additional keyword arguments to be passed to the \n",
    "        RealAmplitudes.\n",
    "\n",
    "    Returns:\n",
    "        QuantumCircuit: The constructed MPS quantum circuit.\n",
    "        \n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qubits = range(num_qubits)\n",
    "    for i, j in zip(qubits[:-1], qubits[1:]):\n",
    "        block = RealAmplitudes(2, parameter_prefix=f\"{parameter_prefix}_{i},{j}\", **kwargs)\n",
    "        qc.compose(block, [i, j], inplace=True)\n",
    "        # if i < num_qubits - 2:\n",
    "        #     qc.barrier()\n",
    "    return qc\n",
    "\n",
    "MPS(4).draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwoLocalMPS(num_qubits,parameter_prefix=\"θ\", **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a Matrix Product State (MPS) quantum circuit using TwoLocal.\n",
    "\n",
    "    Args:\n",
    "        num_qubits (int): The number of qubits in the circuit.\n",
    "        **kwargs: Additional keyword arguments to be passed to TwoLocal.\n",
    "\n",
    "    Returns:\n",
    "        QuantumCircuit: The constructed MPS quantum circuit.\n",
    "        \n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qubits = range(num_qubits)\n",
    "    for i, j in zip(qubits[:-1], qubits[1:]):\n",
    "        block = TwoLocal(\n",
    "            2,\n",
    "            rotation_blocks='rx',\n",
    "            entanglement='reverse_linear',\n",
    "            entanglement_blocks='cx',\n",
    "            parameter_prefix=f'{parameter_prefix}_{i},{j}',\n",
    "            insert_barriers=True,\n",
    "            **kwargs\n",
    "\n",
    "        )\n",
    "        qc.compose(block, [i, j], inplace=True)\n",
    "    return qc\n",
    "\n",
    "TwoLocalMPS(4).draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ansatz(num_qubits, parameter_prefix=\"θ\", ansatz_type=\"MPS\", **kwargs):\n",
    "    \"\"\"\n",
    "    Crea un ansatz del tipo specificato.\n",
    "    \n",
    "    Args:\n",
    "        num_qubits (int): Numero di qubit.\n",
    "        parameter_prefix (str): Prefisso per i parametri.\n",
    "        ansatz_type (str): Tipo di ansatz - \"MPS\" (RealAmplitudes) o \"TwoLocalMPS\" (TwoLocal).\n",
    "        **kwargs: Argomenti aggiuntivi passati alla funzione ansatz.\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Il circuito ansatz.\n",
    "    \"\"\"\n",
    "    if ansatz_type == \"MPS\":\n",
    "        return MPS(num_qubits, parameter_prefix=parameter_prefix, **kwargs)\n",
    "    elif ansatz_type == \"TwoLocalMPS\":\n",
    "        return TwoLocalMPS(num_qubits, parameter_prefix=parameter_prefix, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Ansatz type '{ansatz_type}' non supportato. Usa 'MPS' o 'TwoLocalMPS'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIRCUITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definizione del circuito originale ( Con RealAmplitudes + Pooling)\n",
    "\n",
    "Decommentare l'encoding necessario per poter effettuare gli esperimenti opportuni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = yz_angles_encoding(8, param_name=\"e\")\n",
    "#encoding = hRyRx_encoding(8)\n",
    "#encoding= hRyRz_encoding(8)\n",
    "#encoding = RyRx_encoding(8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ansatz = QuantumCircuit(4)\n",
    "ansatz.barrier()\n",
    "############################2#######################\n",
    "ansatz = ansatz.compose(RealAmplitudes(num_qubits=4, reps=1, name=\"Layer1\", parameter_prefix=\"l1\"))\n",
    "ansatz.barrier()\n",
    "############################3#######################\n",
    "ansatz = ansatz.compose(pooling_layer(4, \"pool2\"))\n",
    "ansatz.barrier()\n",
    "############################4#######################\n",
    "ansatz = ansatz.compose(RealAmplitudes(num_qubits=2, reps=1, name=\"Layer2\",parameter_prefix=\"l2\"), qubits=[2,3])\n",
    "ansatz.barrier()\n",
    "\n",
    "ansatz.decompose().draw(output=\"mpl\")\n",
    "\n",
    "\n",
    "qnn = QuantumCircuit(4).compose(encoding).compose(ansatz)\n",
    "\n",
    "display(qnn.decompose().draw(\"mpl\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuito con TN (solo MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = yz_angles_encoding(8, param_name=\"e\")\n",
    "\n",
    "\n",
    "qnn = QuantumCircuit(4).compose(encoding)\n",
    "\n",
    "# Aggiunta dell'MPS al circuito\n",
    "ansatz = MPS(num_qubits=4, parameter_prefix=\"mps\")\n",
    "qnn = qnn.compose(ansatz)\n",
    "\n",
    "# Visualizzazione del circuito\n",
    "display(qnn.decompose().draw(\"mpl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoding parameters:\", len(encoding.parameters))\n",
    "print(\"Ansatz parameters:\", len(ansatz.parameters))\n",
    "print(\"Total parameters in circuit:\", len(qnn.parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard reuploading circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "import math\n",
    "\n",
    "def build_mps_reuploading_model(n_features, layers=2, fixed_encoder=None, ansatz_type=\"MPS\"):\n",
    "    \"\"\"\n",
    "    Costruisce un classificatore Re-uploading usando combinazioni di encoder e ansatz.\n",
    "    \n",
    "    Args:\n",
    "        n_features (int): Numero di dati in input.\n",
    "        layers (int): Quante volte ripetere la sequenza [Encoder -> Ansatz] (default=2).\n",
    "        ansatz_type (str): Tipo di ansatz - \"MPS\" o \"TwoLocalMPS\" (default=\"MPS\").\n",
    "        fixed_encoder (str): Se None, cicla tra i tre encoder (yz, RxRy, RzRx).\n",
    "                            Se specificato, usa lo stesso encoder per tutti i layer.\n",
    "                            Opzioni: \"yz\", \"RxRy\", \"RzRx\", \"RxRz\", \"RyRx\" (default=None).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcoliamo il numero di qubit necessari\n",
    "    n_qubits = math.ceil(n_features / 2)\n",
    "    \n",
    "    # Inizializziamo il circuito finale\n",
    "    full_qc = QuantumCircuit(n_qubits, name=f\"{ansatz_type} Re-uploading Classifier\")\n",
    "    \n",
    "    # Creiamo i parametri di input UNA VOLTA SOLA (condivisi tra tutti i layer)\n",
    "    input_params = ParameterVector(\"x\", n_features)\n",
    "    \n",
    "    # Ciclo di Re-uploading (Sandwich)\n",
    "    for layer in range(layers):\n",
    "        \n",
    "        # --- A. Aggiungiamo l'Encoder ---\n",
    "        if fixed_encoder is None:\n",
    "            # Comportamento ciclico: cicla tra i tre encoder\n",
    "            encoder_choice = layer % 3\n",
    "            \n",
    "            if encoder_choice == 0:\n",
    "                # YZ encoding (RY, RZ)\n",
    "                encoder_block = yz_angles_encoding(n_features, params=input_params)\n",
    "            elif encoder_choice == 1:\n",
    "                # RX-RY encoding (RX, RY)\n",
    "                encoder_block = RxRy_encoding(n_features, params=input_params)\n",
    "            else:\n",
    "                # RZ-RX encoding (RZ, RX)\n",
    "                encoder_block = RzRx_encoding(n_features, params=input_params)\n",
    "        else:\n",
    "            # Usa l'encoder fisso specificato per tutti i layer\n",
    "            if fixed_encoder == \"yz\":\n",
    "                encoder_block = yz_angles_encoding(n_features, params=input_params)\n",
    "            elif fixed_encoder == \"RxRy\":\n",
    "                encoder_block = RxRy_encoding(n_features, params=input_params)\n",
    "            elif fixed_encoder == \"RxRz\":\n",
    "                encoder_block = RxRz_encoding(n_features, params=input_params)\n",
    "            elif fixed_encoder == \"RyRx\":\n",
    "                encoder_block = RyRx_encoding(n_features, params=input_params)\n",
    "            elif fixed_encoder == \"RzRx\":\n",
    "                encoder_block = RzRx_encoding(n_features, params=input_params)\n",
    "            else:\n",
    "                raise ValueError(f\"Encoder '{fixed_encoder}' non supportato. \"\n",
    "                               \"Usa: 'yz', 'RxRy', 'RxRz', 'RyRx', 'RzRx' o None per il comportamento ciclico.\")\n",
    "\n",
    "        full_qc.compose(encoder_block, inplace=True)\n",
    "        full_qc.barrier()\n",
    "        \n",
    "        # --- B. Aggiungiamo l'Ansatz (MPS o TwoLocalMPS) ---\n",
    "        ansatz_block = create_ansatz(n_qubits, parameter_prefix=f\"w_L{layer}\", ansatz_type=ansatz_type, reps=1)\n",
    "        full_qc.compose(ansatz_block, inplace=True)\n",
    "        \n",
    "        full_qc.barrier()\n",
    "        full_qc.barrier()\n",
    "\n",
    "    return full_qc\n",
    "\n",
    "# --- TEST ---\n",
    "# Supponiamo 8 feature e 2 livelli di profondità\n",
    "\n",
    "ansatz_type = \"MPS\"  # Scegli tra \"MPS\" e \"TwoLocalMPS\"\n",
    "fixed_enc = None  # Scegli tra None (ciclico), \"yz\", \"RxRy\", \"RxRz\", \"RyRx\", \"RzRx\"\n",
    "qnn_std_reupload = build_mps_reuploading_model(n_features=8, layers=2, fixed_encoder=fixed_enc, ansatz_type=ansatz_type)\n",
    "\n",
    "# Disegniamo il risultato\n",
    "print(f\"Circuito con encoder: {fixed_enc}, ansatz: {ansatz_type}\")\n",
    "qnn_std_reupload.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial reuploading circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_partial_mps_model(n_features, layers=3, features_per_layer=4, fixed_encoder=\"yz\", ansatz_type=\"MPS\"):\n",
    "    \"\"\"\n",
    "    Costruisce un classificatore Sequential Partial con finestra scorrevole.\n",
    "    Ogni layer carica feature DIVERSE su qubit sovrapposti in modo sequenziale.\n",
    "    \n",
    "    Esempio con 4 qubit, 3 layer, 4 feature/layer:\n",
    "    - Layer 0: feature 0-3  → qubit (0,1)\n",
    "    - Layer 1: feature 4-7  → qubit (1,2)  \n",
    "    - Layer 2: feature 8-11 → qubit (2,3)\n",
    "    Totale: 12 feature mappate!\n",
    "    \n",
    "    Args:\n",
    "        n_features (int): Numero TOTALE di feature da mappare (aumentato rispetto al caso standard).\n",
    "        layers (int): Numero di blocchi [Encoding -> Ansatz].\n",
    "        features_per_layer (int): Quante feature caricare in ogni layer (deve essere pari).\n",
    "        ansatz_type (str): Tipo di ansatz - \"MPS\" o \"TwoLocalMPS\" (default=\"MPS\").\n",
    "        fixed_encoder (str): Encoder da usare per tutti i layer.\n",
    "                            Opzioni: \"yz\", \"RxRy\", \"RxRz\", \"RyRx\", \"RzRx\" (default=\"yz\").\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assicurati che le feature per layer siano un numero pari\n",
    "    if features_per_layer % 2 != 0:\n",
    "        features_per_layer += 1\n",
    "        print(f\"Adattato features_per_layer a {features_per_layer} per l'encoding a coppie.\")\n",
    "    \n",
    "    # Validazione encoder\n",
    "    valid_encoders = {\"yz\", \"RxRy\", \"RxRz\", \"RyRx\", \"RzRx\"}\n",
    "    if fixed_encoder not in valid_encoders:\n",
    "        raise ValueError(f\"Encoder '{fixed_encoder}' non supportato. \"\n",
    "                        f\"Usa: {valid_encoders}\")\n",
    "    \n",
    "    # Mappa encoder a coppie di gate (gate1, gate2)\n",
    "    encoder_gates = {\n",
    "        \"yz\": (\"ry\", \"rz\"),\n",
    "        \"RxRy\": (\"rx\", \"ry\"),\n",
    "        \"RxRz\": (\"rx\", \"rz\"),\n",
    "        \"RyRx\": (\"ry\", \"rx\"),\n",
    "        \"RzRx\": (\"rz\", \"rx\"),\n",
    "    }\n",
    "    gate1_name, gate2_name = encoder_gates[fixed_encoder]\n",
    "    \n",
    "    # Calcolo del numero di qubit necessari per la finestra scorrevole\n",
    "    # Primo layer usa features_per_layer/2 qubit, poi ogni layer aggiunge 1 qubit\n",
    "    n_qubits_per_encoding = math.ceil(features_per_layer / 2)\n",
    "    n_qubits = n_qubits_per_encoding + (layers - 1)\n",
    "    \n",
    "    # Calcola il numero totale di feature che verranno mappate\n",
    "    total_features_mapped = layers * features_per_layer\n",
    "    \n",
    "    if n_features < total_features_mapped:\n",
    "        print(f\"ATTENZIONE: Con {layers} layer e {features_per_layer} feature/layer,\")\n",
    "        print(f\"verranno mappate {total_features_mapped} feature totali.\")\n",
    "        print(f\"Aumenta n_features a {total_features_mapped} o riduci layers/features_per_layer.\")\n",
    "    \n",
    "    # Inizializziamo il circuito\n",
    "    full_qc = QuantumCircuit(n_qubits, name=f\"Sequential Sliding Window ({fixed_encoder})\")\n",
    "    input_params = ParameterVector(\"x\", total_features_mapped)\n",
    "    \n",
    "    print(f\"Configurazione: {n_qubits} qubit, {layers} layer, {features_per_layer} feature/layer, encoder: {fixed_encoder}\")\n",
    "    print(f\"Feature totali mappate: {total_features_mapped}\")\n",
    "    \n",
    "    for layer in range(layers):\n",
    "        \n",
    "        # --- A. ENCODER SEQUENZIALE A FINESTRA SCORREVOLE ---\n",
    "        # Calcola l'offset dei qubit per questo layer (finestra scorrevole)\n",
    "        qubit_offset = layer\n",
    "        \n",
    "        # Calcola l'indice di partenza delle feature per questo layer (sequenziale, non ciclico)\n",
    "        start_feature_idx = layer * features_per_layer\n",
    "        \n",
    "        # Carica il blocco di feature sui qubit della finestra corrente\n",
    "        feature_idx = 0\n",
    "        for i in range(n_qubits_per_encoding):\n",
    "            qubit_target = qubit_offset + i\n",
    "            \n",
    "            p_idx_1 = start_feature_idx + feature_idx\n",
    "            p_idx_2 = start_feature_idx + feature_idx + 1\n",
    "            \n",
    "            # Applica i gate solo se l'indice è valido\n",
    "            if p_idx_1 < total_features_mapped:\n",
    "                getattr(full_qc, gate1_name)(input_params[p_idx_1], qubit_target)\n",
    "            if p_idx_2 < total_features_mapped:\n",
    "                getattr(full_qc, gate2_name)(input_params[p_idx_2], qubit_target)\n",
    "            \n",
    "            feature_idx += 2\n",
    "        \n",
    "        full_qc.barrier()\n",
    "        \n",
    "        # --- B. ANSATZ (MPS o TwoLocalMPS) solo sui qubit popolati in questo layer ---\n",
    "        # Calcola i qubit usati in questo layer\n",
    "        qubits_in_layer = list(range(qubit_offset, qubit_offset + n_qubits_per_encoding))\n",
    "        ansatz_block = create_ansatz(n_qubits_per_encoding, parameter_prefix=f\"w_L{layer}\", ansatz_type=ansatz_type, reps=1)\n",
    "        full_qc.compose(ansatz_block, qubits_in_layer, inplace=True)\n",
    "        \n",
    "        full_qc.barrier()\n",
    "        full_qc.barrier()\n",
    "    \n",
    "    return full_qc\n",
    "\n",
    "# --- TEST ---\n",
    "# Con 3 layer e 4 feature/layer, mappiamo 12 feature totali su 4 qubit\n",
    "# Cambia ansatz_type=\"TwoLocalMPS\" per usare TwoLocal invece di RealAmplitudes\n",
    "# Scegli l'encoder: \"yz\", \"RxRy\", \"RxRz\", \"RyRx\", \"RzRx\"\n",
    "print(\"=== Test Sequential Sliding Window ===\")\n",
    "qnn_partial_reupload = build_partial_mps_model(n_features=12, layers=3, features_per_layer=4, \n",
    "                                               fixed_encoder=\"RxRz\", ansatz_type=\"TwoLocalMPS\")\n",
    "\n",
    "qnn_partial_reupload.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYBRID NETWORKS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-features per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.787845Z",
     "start_time": "2025-02-25T12:56:31.753308Z"
    }
   },
   "outputs": [],
   "source": [
    "from qiskit.primitives import Sampler\n",
    "def parity(x):\n",
    "    return f\"{bin(x)}\".count(\"1\")\n",
    "\n",
    "# SCELTA DEL CIRCUITO QUANTISTICO\n",
    "qnn_circuit_8 = build_mps_reuploading_model(n_features=8, layers=2, ansatz_type=\"TwoLocalMPS\")\n",
    "\n",
    "# Separa i parametri: i primi 8 sono input, il resto sono weight dall'ansatz MPS\n",
    "input_ps_8 = list(qnn_circuit_8.parameters)[:8]\n",
    "weight_ps_8 = list(qnn_circuit_8.parameters)[8:]\n",
    "\n",
    "qmodel_8 = SamplerQNN(\n",
    "    circuit=qnn_circuit_8,\n",
    "    input_params=input_ps_8,\n",
    "    weight_params=weight_ps_8,\n",
    "    input_gradients=True\n",
    ")\n",
    "\n",
    "\n",
    "class HybridRegressorConvNet8Features(nn.Module):\n",
    "    def __init__(self, qm1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5,5)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.reduction = nn.Sequential(\n",
    "            nn.Linear(in_features=576, out_features=120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=120, out_features=8)\n",
    "        )\n",
    "     \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(in_features=16, out_features=16),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.q1 = TorchConnector(qm1)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        #Per encoding con HRyRz\n",
    "        #x = x * (math.pi / 2)\n",
    "        #x= self.q1(x)\n",
    "        \n",
    "        x = self.q1(x) * 100\n",
    "\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "distance_model_8 = HybridRegressorConvNet8Features(qmodel_8)\n",
    "distance_model_8 = distance_model_8.to(device)\n",
    "summary(distance_model_8, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12-features per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.primitives import Sampler\n",
    "\n",
    "# SCELTA DEL CIRCUITO QUANTISTICO PER 12 FEATURES\n",
    "qnn_circuit_12 = build_partial_mps_model(n_features=12, layers=3, features_per_layer=4, ansatz_type=\"TwoLocalMPS\")\n",
    "\n",
    "# Separa i parametri: i primi 12 sono input, il resto sono weight dall'ansatz RealAmplitudes\n",
    "input_ps_12 = list(qnn_circuit_12.parameters)[:12]\n",
    "weight_ps_12 = list(qnn_circuit_12.parameters)[12:]\n",
    "\n",
    "qmodel_12 = SamplerQNN(\n",
    "    circuit=qnn_circuit_12,\n",
    "    input_params=input_ps_12,\n",
    "    weight_params=weight_ps_12,\n",
    "    input_gradients=True\n",
    ")\n",
    "\n",
    "\n",
    "class HybridRegressorConvNet12Features(nn.Module):\n",
    "    def __init__(self, qm1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5,5)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Riduzione a 12 feature invece di 8\n",
    "        self.reduction = nn.Sequential(\n",
    "            nn.Linear(in_features=576, out_features=120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=120, out_features=12)\n",
    "        )\n",
    "     \n",
    "        # Output layer (da 16 a 16 qubit output)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(in_features=16, out_features=16),\n",
    "        )\n",
    "\n",
    "        self.q1 = TorchConnector(qm1)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.reduction(x)\n",
    "        \n",
    "        x = self.q1(x) * 100\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "distance_model_12 = HybridRegressorConvNet12Features(qmodel_12)\n",
    "distance_model_12 = distance_model_12.to(device)\n",
    "summary(distance_model_12, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.830824Z",
     "start_time": "2025-02-25T12:56:31.827162Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 8\n",
    "if n_features == 8:\n",
    "    model_to_train = qmodel_8\n",
    "    distance_model = HybridRegressorConvNet8Features(model_to_train)\n",
    "else:\n",
    "    model_to_train = qmodel_12\n",
    "    distance_model = HybridRegressorConvNet12Features(model_to_train)\n",
    "training_dataloader = DataLoader(t, batch_size=8, shuffle=True)\n",
    "val_data_loader = DataLoader(v, batch_size=8, shuffle=True)\n",
    "distance_model = distance_model.to(device)\n",
    "optimizer = torch.optim.SGD(distance_model.parameters(), lr=1e-2)\n",
    "loss = torch.nn.TripletMarginLoss(margin=2)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.928352Z",
     "start_time": "2025-02-25T12:56:31.855516Z"
    }
   },
   "outputs": [],
   "source": [
    "model_to_train.circuit.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.954138Z",
     "start_time": "2025-02-25T12:56:31.950554Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, criterion, train_data_loader, val_data_loader, device, validation_step=False, print_at=1, seed= 42):\n",
    "\n",
    "    \n",
    "    set_seed(seed)\n",
    "    for epoch in range(epochs):\n",
    "        set_seed(seed + epoch)\n",
    "        \n",
    "        prnt = (epoch % print_at) != 0 if epoch!=(epochs-1) else False\n",
    "\n",
    "\n",
    "        if not(prnt):\n",
    "            print(f\"{color.BOLD}Epoch {color.END}{epoch+1}\")\n",
    "\n",
    "        ### --> Training Phase\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        \n",
    "\n",
    "        for anchor, positive, negative in tqdm(train_data_loader, disable=prnt):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "\n",
    "            e_A = model(anchor)\n",
    "            e_P = model(positive)\n",
    "            e_N = model(negative)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            loss = criterion(e_A, e_P, e_N)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss \n",
    "            train_samples += positive.size(0)\n",
    "\n",
    "\n",
    "        train_loss /= len(train_data_loader)\n",
    "        \n",
    "        if not(prnt):\n",
    "            print(f\"TRAINING   -> Loss: {train_loss:2.6f}\")\n",
    "            print(\"\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop with early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.978202Z",
     "start_time": "2025-02-25T12:56:31.973369Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, criterion, train_data_loader, val_data_loader, device, \n",
    "          validation_step=False, print_at=1, early_stopping_patience=3, \n",
    "          min_delta=0.001, restore_best_weights=True, seed=42, \n",
    "          checkpoint_interval=100, checkpoint_path=\"model_results/triplet_loss\"):\n",
    "    \"\"\"\n",
    "    Funzione di training con checkpoint automatici ogni N epoche.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_interval: Salva un checkpoint ogni N epoche (default=100)\n",
    "        checkpoint_path: Percorso base per salvare i checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        model: Modello addestrato\n",
    "        checkpoint_epochs: Lista delle epoche in cui sono stati salvati checkpoint\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    set_seed(seed)\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    best_model_state = None\n",
    "    checkpoint_epochs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        set_seed(seed + epoch)\n",
    "        prnt = (epoch % print_at) != 0 if epoch != (epochs-1) else False\n",
    "\n",
    "        # Inizializza early_stopping_triggered all'inizio di ogni epoch\n",
    "        early_stopping_triggered = False\n",
    "\n",
    "        if not prnt:\n",
    "            print(f\"{color.BOLD}Epoch {color.END}{epoch+1}\")\n",
    "\n",
    "        ### --> Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        for anchor, positive, negative in tqdm(train_data_loader, disable=prnt):\n",
    "            optimizer.zero_grad()\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            e_A = model(anchor)\n",
    "            e_P = model(positive)\n",
    "            e_N = model(negative)\n",
    "\n",
    "            loss = criterion(e_A, e_P, e_N)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_samples += positive.size(0)\n",
    "\n",
    "        train_loss /= len(train_data_loader)\n",
    "        \n",
    "        ### --> Validation Phase (solo se richiesto)\n",
    "        val_loss = 0.0\n",
    "        if validation_step and not prnt:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for anchor, positive, negative in val_data_loader:\n",
    "                    anchor = anchor.to(device)\n",
    "                    positive = positive.to(device)\n",
    "                    negative = negative.to(device)\n",
    "\n",
    "                    e_A = model(anchor)\n",
    "                    e_P = model(positive)\n",
    "                    e_N = model(negative)\n",
    "\n",
    "                    loss = criterion(e_A, e_P, e_N)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                val_loss /= len(val_data_loader)\n",
    "\n",
    "        ### --> Early Stopping Logic\n",
    "        if validation_step and not prnt:\n",
    "            # Controlla miglioramento\n",
    "            if (best_loss - val_loss) > min_delta:\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                no_improvement_count = 0\n",
    "                # Salva lo stato del modello migliore\n",
    "                if restore_best_weights:\n",
    "                    print(\"Best epoch: \", best_epoch)\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= early_stopping_patience:\n",
    "                    print(f\"\\n{color.RED}Early stopping at epoch {epoch+1}{color.END}\")\n",
    "                    early_stopping_triggered = True\n",
    "\n",
    "        ### --> Stampa risultati\n",
    "        if not prnt:\n",
    "            log_str = f\"TRAINING   -> Loss: {train_loss:2.6f}\"\n",
    "            if validation_step:\n",
    "                log_str += f\" | VALIDATION -> Loss: {val_loss:2.6f}\"\n",
    "            print(log_str)\n",
    "            print(\"\")\n",
    "\n",
    "        ### --> Salva checkpoint ogni checkpoint_interval epoche\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_filename = f\"{checkpoint_path}/checkpoint_epoch_{epoch+1}.pt\"\n",
    "            torch.save(model.state_dict(), checkpoint_filename)\n",
    "            checkpoint_epochs.append(epoch + 1)\n",
    "            print(f\"{color.GREEN}✓ Checkpoint salvato: {checkpoint_filename}{color.END}\")\n",
    "            print(\"\")\n",
    "\n",
    "        ### --> Interrompi il training se early stopping\n",
    "        if early_stopping_triggered:\n",
    "            break\n",
    "\n",
    "    ### --> Ripristina i migliori pesi se richiesto\n",
    "    if restore_best_weights and best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"{color.GREEN}Restored best model weights{color.END}\")\n",
    "\n",
    "    return model, checkpoint_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T14:14:49.161048Z",
     "start_time": "2025-02-25T12:56:32.035203Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURAZIONE CHECKPOINT\n",
    "CHECKPOINT_PATH = \"model_results/triplet_loss/Standard_TwoLocalMPS\"\n",
    "CHECKPOINT_INTERVAL = 100  # Salva ogni 100 epoche\n",
    "\n",
    "#Train senza Early Stopping\n",
    "distance_model, checkpoint_epochs = train(\n",
    "    model=distance_model,\n",
    "    epochs=200,\n",
    "    optimizer=optimizer,\n",
    "    criterion=loss,\n",
    "    train_data_loader=training_dataloader,\n",
    "    val_data_loader=None,\n",
    "    validation_step=False,\n",
    "    device=device,\n",
    "    print_at=1,\n",
    "    seed=42,\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "    checkpoint_path=CHECKPOINT_PATH\n",
    ")\n",
    "\n",
    "print(f\"\\n{color.BOLD}Training completato!{color.END}\")\n",
    "print(f\"Checkpoint salvati alle epoche: {checkpoint_epochs}\")\n",
    "\n",
    "\n",
    "#Train con Early Stopping\n",
    "# distance_model, checkpoint_epochs = train(\n",
    "#     distance_model,\n",
    "#     epochs=200,\n",
    "#     optimizer=optimizer,\n",
    "#     criterion=loss,\n",
    "#     train_data_loader=training_dataloader,\n",
    "#     val_data_loader=val_data_loader,\n",
    "#     device=device,\n",
    "#     validation_step=True,\n",
    "#     early_stopping_patience=4,\n",
    "#     min_delta=0.005,\n",
    "#     seed=42,\n",
    "#     checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "#     checkpoint_path=CHECKPOINT_PATH\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALUTAZIONE AUTOMATICA CHECKPOINT SUL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def evaluate_checkpoint_on_test(checkpoint_path, model_template, test_dataset, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    Carica un checkpoint e valuta le performance sul test set.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path del checkpoint da caricare\n",
    "        model_template: Modello template (con l'architettura corretta)\n",
    "        test_dataset: Dataset di test\n",
    "        device: Device (cpu/cuda)\n",
    "        batch_size: Batch size per il test\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary con tutte le metriche calcolate\n",
    "    \"\"\"\n",
    "    # Carica il checkpoint\n",
    "    model_template.load_state_dict(torch.load(checkpoint_path))\n",
    "    model_template.eval()\n",
    "    \n",
    "    # Genera gli embedding per il test set\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_embedding_data = np.empty((0, 16))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for anchor, _, _ in tqdm(test_loader, desc=f\"Generating embeddings\"):\n",
    "            anchor = anchor.to(device)\n",
    "            embeddings = model_template(anchor).to(\"cpu\").numpy()\n",
    "            test_embedding_data = np.concatenate((test_embedding_data, embeddings), axis=0)\n",
    "    \n",
    "    # Clustering con Agglomerative\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=10, linkage=\"average\")\n",
    "    agg_prediction = agg_clustering.fit_predict(test_embedding_data)\n",
    "    agg_silhouette, agg_purity = evaluate_clustering_table(test_embedding_data, agg_prediction, test_dataset.target)\n",
    "    \n",
    "    # Clustering con KMeans\n",
    "    kmeans_clustering = KMeans(n_clusters=10, init=\"k-means++\", n_init=10, random_state=42)\n",
    "    kmeans_prediction = kmeans_clustering.fit_predict(test_embedding_data)\n",
    "    kmeans_silhouette, kmeans_purity = evaluate_clustering_table(test_embedding_data, kmeans_prediction, test_dataset.target)\n",
    "    \n",
    "    return {\n",
    "        'checkpoint': checkpoint_path,\n",
    "        'test_embedding': test_embedding_data,\n",
    "        'agg_silhouette': agg_silhouette,\n",
    "        'agg_purity': agg_purity,\n",
    "        'agg_prediction': agg_prediction,\n",
    "        'kmeans_silhouette': kmeans_silhouette,\n",
    "        'kmeans_purity': kmeans_purity,\n",
    "        'kmeans_prediction': kmeans_prediction\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_all_checkpoints(checkpoint_path, model_template, test_dataset, device):\n",
    "    \"\"\"\n",
    "    Valuta tutti i checkpoint salvati durante il training.\n",
    "    \n",
    "    Returns:\n",
    "        Lista di dizionari con i risultati per ogni checkpoint\n",
    "    \"\"\"\n",
    "    # Trova tutti i checkpoint\n",
    "    checkpoint_files = sorted(glob.glob(f\"{checkpoint_path}/checkpoint_epoch_*.pt\"))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(f\"{color.RED}Nessun checkpoint trovato in {checkpoint_path}{color.END}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n{color.BOLD}=== VALUTAZIONE CHECKPOINT SUL TEST SET ==={color.END}\")\n",
    "    print(f\"Trovati {len(checkpoint_files)} checkpoint da valutare\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for checkpoint_file in checkpoint_files:\n",
    "        # Estrai il numero di epoca dal filename\n",
    "        epoch_num = int(checkpoint_file.split('_')[-1].replace('.pt', ''))\n",
    "        print(f\"\\n{color.CYAN}{'='*60}{color.END}\")\n",
    "        print(f\"{color.BOLD}Checkpoint Epoca {epoch_num}{color.END}\")\n",
    "        print(f\"{color.CYAN}{'='*60}{color.END}\\n\")\n",
    "        \n",
    "        result = evaluate_checkpoint_on_test(checkpoint_file, model_template, test_dataset, device)\n",
    "        result['epoch'] = epoch_num\n",
    "        results.append(result)\n",
    "        \n",
    "        # Stampa i risultati\n",
    "        print(f\"\\n{color.GREEN}Risultati Test Set (10,000 campioni):{color.END}\")\n",
    "        print(f\"  Agglomerative Clustering:\")\n",
    "        print(f\"    - Silhouette: {result['agg_silhouette']:.4f}\")\n",
    "        print(f\"    - Purity:     {result['agg_purity']:.4f}\")\n",
    "        print(f\"  KMeans Clustering:\")\n",
    "        print(f\"    - Silhouette: {result['kmeans_silhouette']:.4f}\")\n",
    "        print(f\"    - Purity:     {result['kmeans_purity']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Prepara il test dataset\n",
    "t_test = MNIST_Distance_Dataset_Triplet_Loss(X_test, y_test)\n",
    "\n",
    "# Valuta tutti i checkpoint\n",
    "checkpoint_results = evaluate_all_checkpoints(\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    model_template=distance_model,\n",
    "    test_dataset=t_test,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabella riassuntiva dei risultati\n",
    "print(f\"\\n{color.BOLD}{'='*80}{color.END}\")\n",
    "print(f\"{color.BOLD}TABELLA RIASSUNTIVA - CONFRONTO CHECKPOINT{color.END}\")\n",
    "print(f\"{color.BOLD}{'='*80}{color.END}\\n\")\n",
    "\n",
    "print(f\"{'Epoca':<10} {'Agglomerative':<30} {'KMeans':<30}\")\n",
    "print(f\"{'':10} {'Silhouette':<15} {'Purity':<15} {'Silhouette':<15} {'Purity':<15}\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "for result in checkpoint_results:\n",
    "    print(f\"{result['epoch']:<10} \"\n",
    "          f\"{result['agg_silhouette']:<15.4f} {result['agg_purity']:<15.4f} \"\n",
    "          f\"{result['kmeans_silhouette']:<15.4f} {result['kmeans_purity']:<15.4f}\")\n",
    "\n",
    "print(f\"{'-'*80}\\n\")\n",
    "\n",
    "# Identifica il miglior checkpoint per ogni metrica\n",
    "best_agg_sil = max(checkpoint_results, key=lambda x: x['agg_silhouette'])\n",
    "best_agg_pur = max(checkpoint_results, key=lambda x: x['agg_purity'])\n",
    "best_km_sil = max(checkpoint_results, key=lambda x: x['kmeans_silhouette'])\n",
    "best_km_pur = max(checkpoint_results, key=lambda x: x['kmeans_purity'])\n",
    "\n",
    "print(f\"{color.GREEN}Migliori performance:{color.END}\")\n",
    "print(f\"  Agglomerative Silhouette: Epoca {best_agg_sil['epoch']} ({best_agg_sil['agg_silhouette']:.4f})\")\n",
    "print(f\"  Agglomerative Purity:     Epoca {best_agg_pur['epoch']} ({best_agg_pur['agg_purity']:.4f})\")\n",
    "print(f\"  KMeans Silhouette:        Epoca {best_km_sil['epoch']} ({best_km_sil['kmeans_silhouette']:.4f})\")\n",
    "print(f\"  KMeans Purity:            Epoca {best_km_pur['epoch']} ({best_km_pur['kmeans_purity']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafici comparativi delle metriche\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Confronto Performance Checkpoint sul Test Set', fontsize=16, weight='bold')\n",
    "\n",
    "epochs = [r['epoch'] for r in checkpoint_results]\n",
    "\n",
    "# Agglomerative Silhouette\n",
    "axes[0, 0].plot(epochs, [r['agg_silhouette'] for r in checkpoint_results], \n",
    "                marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Epoca', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[0, 0].set_title('Agglomerative Clustering - Silhouette', fontsize=14, weight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(epochs)\n",
    "\n",
    "# Agglomerative Purity\n",
    "axes[0, 1].plot(epochs, [r['agg_purity'] for r in checkpoint_results], \n",
    "                marker='s', linewidth=2, markersize=8, color='seagreen')\n",
    "axes[0, 1].set_xlabel('Epoca', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Purity Score', fontsize=12)\n",
    "axes[0, 1].set_title('Agglomerative Clustering - Purity', fontsize=14, weight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(epochs)\n",
    "\n",
    "# KMeans Silhouette\n",
    "axes[1, 0].plot(epochs, [r['kmeans_silhouette'] for r in checkpoint_results], \n",
    "                marker='^', linewidth=2, markersize=8, color='coral')\n",
    "axes[1, 0].set_xlabel('Epoca', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1, 0].set_title('KMeans Clustering - Silhouette', fontsize=14, weight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(epochs)\n",
    "\n",
    "# KMeans Purity\n",
    "axes[1, 1].plot(epochs, [r['kmeans_purity'] for r in checkpoint_results], \n",
    "                marker='D', linewidth=2, markersize=8, color='mediumpurple')\n",
    "axes[1, 1].set_xlabel('Epoca', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Purity Score', fontsize=12)\n",
    "axes[1, 1].set_title('KMeans Clustering - Purity', fontsize=14, weight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(epochs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione t-SNE per tutti i checkpoint confrontati con KMeans\n",
    "num_checkpoints = len(checkpoint_results)\n",
    "fig, axes = plt.subplots(1, num_checkpoints, figsize=(10 * num_checkpoints, 8))\n",
    "\n",
    "if num_checkpoints == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "fig.suptitle('Confronto Clustering KMeans tra Checkpoint (t-SNE Visualization)', \n",
    "             fontsize=16, weight='bold')\n",
    "\n",
    "for idx, result in enumerate(checkpoint_results):\n",
    "    # Applica t-SNE agli embedding\n",
    "    reduction_model = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    vis_embed = reduction_model.fit_transform(result['test_embedding'])\n",
    "    \n",
    "    # Visualizza il clustering\n",
    "    axes[idx].scatter(vis_embed[:, 0], vis_embed[:, 1], \n",
    "                     c=result['kmeans_prediction'], cmap='Dark2', alpha=0.6, s=10)\n",
    "    axes[idx].set_title(f\"Epoca {result['epoch']}\\n\"\n",
    "                       f\"Silhouette: {result['kmeans_silhouette']:.4f} | \"\n",
    "                       f\"Purity: {result['kmeans_purity']:.4f}\",\n",
    "                       fontsize=12, weight='bold')\n",
    "    axes[idx].set_xlabel('t-SNE Dim 1')\n",
    "    axes[idx].set_ylabel('t-SNE Dim 2')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva i risultati in un file per riferimento\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results_summary = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'test_samples': len(t_test),\n",
    "    'checkpoints': []\n",
    "}\n",
    "\n",
    "for result in checkpoint_results:\n",
    "    results_summary['checkpoints'].append({\n",
    "        'epoch': result['epoch'],\n",
    "        'agglomerative': {\n",
    "            'silhouette': float(result['agg_silhouette']),\n",
    "            'purity': float(result['agg_purity'])\n",
    "        },\n",
    "        'kmeans': {\n",
    "            'silhouette': float(result['kmeans_silhouette']),\n",
    "            'purity': float(result['kmeans_purity'])\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Salva in JSON\n",
    "results_file = f\"{CHECKPOINT_PATH}/evaluation_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n{color.GREEN}✓ Risultati salvati in: {results_file}{color.END}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
