{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.533159Z",
     "start_time": "2025-02-25T12:56:28.526193Z"
    }
   },
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "import sklearn.metrics.cluster as cluster_metrics\n",
    "import numpy as np\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
    "from sklearn.manifold import TSNE\n",
    "from qiskit.circuit.library import RealAmplitudes, TwoLocal\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import torch\n",
    "from torch import nn\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEED\n",
    "\n",
    "Configuration of the random seed for experimental reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.583214Z",
     "start_time": "2025-02-25T12:56:28.573346Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set all needed seeds\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility across all random number generators.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): Seed value for reproducibility\n",
    "    \"\"\"\n",
    "    # Python random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy random generator\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch random generator\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # CUDA deterministic operations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Qiskit determinism\n",
    "    algorithm_globals.random_seed = seed\n",
    "    \n",
    "    # Python hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITIES\n",
    "\n",
    "Utility functions for metrics evaluation and environment management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.663103Z",
     "start_time": "2025-02-25T12:56:28.658970Z"
    }
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using {color.BOLD}{str(device).upper()}{color.END} acceleration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.687986Z",
     "start_time": "2025-02-25T12:56:28.685260Z"
    }
   },
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the purity score for clustering evaluation.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth labels\n",
    "        y_pred: Predicted cluster labels\n",
    "    \n",
    "    Returns:\n",
    "        float: Purity score\n",
    "    \"\"\"\n",
    "    contingency_matrix = cluster_metrics.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "def evaluate_clustering(X, pred_y, true_y):\n",
    "    \"\"\"\n",
    "    Evaluate clustering performance using silhouette and purity metrics.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        pred_y: Predicted cluster labels\n",
    "        true_y: Ground truth labels\n",
    "    \"\"\"\n",
    "    print(f\"Silhouette: {silhouette_score(X, pred_y):2.3f}\")\n",
    "    print(f\"Purity: {purity_score(true_y, pred_y):2.3f}\")\n",
    "\n",
    "def evaluate_clustering_table(X, pred_y, true_y):\n",
    "    \"\"\"\n",
    "    Evaluate clustering performance and return metrics as tuple.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        pred_y: Predicted cluster labels\n",
    "        true_y: Ground truth labels\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (silhouette_score, purity_score)\n",
    "    \"\"\"\n",
    "    return silhouette_score(X, pred_y), purity_score(true_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET IMPORT\n",
    "\n",
    "Dataset importation and configuration for training and testing procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.788896Z",
     "start_time": "2025-02-25T12:56:28.713256Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# DATASET CONFIGURATION\n",
    "# ===========================\n",
    "# Select dataset: \"MNIST\" or \"FashionMNIST\"\n",
    "DATASET_TYPE = \"MNIST\"  # Change to \"FashionMNIST\" to use Fashion-MNIST\n",
    "\n",
    "INITIAL_TRAINING_SAMPLES = 400\n",
    "\n",
    "\n",
    "MNIST_base_transform = transforms.Compose([\n",
    "transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset loading based on configuration\n",
    "if DATASET_TYPE == \"MNIST\":\n",
    "    MNIST_dataset_train = datasets.MNIST(root=\"quantum_metric_learning-master/data/dataset/MNIST\", train=True, download=True, transform=MNIST_base_transform)\n",
    "    MNIST_dataset_test = datasets.MNIST(root=\"quantum_metric_learning-master/data/dataset/MNIST\", train=False, download=True, transform=MNIST_base_transform)\n",
    "    print(f\"{color.BOLD}Using MNIST dataset{color.END}\")\n",
    "elif DATASET_TYPE == \"FashionMNIST\":\n",
    "    MNIST_dataset_train = datasets.FashionMNIST(root=\"quantum_metric_learning-master/data/dataset/FashionMNIST\", train=True, download=True, transform=MNIST_base_transform)\n",
    "    MNIST_dataset_test = datasets.FashionMNIST(root=\"quantum_metric_learning-master/data/dataset/FashionMNIST\", train=False, download=True, transform=MNIST_base_transform)\n",
    "    print(f\"{color.BOLD}Using Fashion MNIST dataset{color.END}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset type: {DATASET_TYPE}. Choose 'MNIST' or 'FashionMNIST'\")\n",
    "\n",
    "if False:\n",
    "    index01 = MNIST_dataset_train.train_labels <= 1\n",
    "    MNIST_dataset_train.data = MNIST_dataset_train.data[index01]\n",
    "    MNIST_dataset_train.targets = MNIST_dataset_train.targets[index01]\n",
    "\n",
    "    index01_test = MNIST_dataset_test.train_labels <= 1\n",
    "    MNIST_dataset_test.data = MNIST_dataset_test.data[index01_test]\n",
    "    MNIST_dataset_test.targets = MNIST_dataset_test.targets[index01_test]\n",
    "\n",
    "print(f\"FULL DATASET INFORMATION\")\n",
    "print(f\"Image shape: {MNIST_dataset_train.data.shape}\")\n",
    "print(f\"Total training samples: {len(MNIST_dataset_train)}\")\n",
    "print(f\"Total test samples: {len(MNIST_dataset_test)}\")\n",
    "print(\"\")\n",
    "\n",
    "init_train_data, _, init_train_target, _ = train_test_split(\n",
    "    range(len(MNIST_dataset_train)), \n",
    "    MNIST_dataset_train.targets,\n",
    "    random_state=42,\n",
    "    stratify=MNIST_dataset_train.targets,\n",
    "    test_size=len(MNIST_dataset_train)- INITIAL_TRAINING_SAMPLES)\n",
    "\n",
    "X = MNIST_dataset_train.data[init_train_data].numpy().astype(\"float32\") / 255\n",
    "y = MNIST_dataset_train.targets[init_train_data].numpy().astype(\"float32\") \n",
    "\n",
    "print(\"TRAINING DATA INFORMATION\")\n",
    "print(f\"Image shape: {X.shape}\")\n",
    "print(f\"Total training samples: {len(X)}\")\n",
    "print(\"\")\n",
    "\n",
    "'''\n",
    "init_test_data, _, init_test_target, _ = train_test_split(\n",
    "    range(len(MNIST_dataset_test)), \n",
    "    MNIST_dataset_test.targets,\n",
    "    random_state=42,\n",
    "    stratify=MNIST_dataset_test.targets,\n",
    "    test_size=len(MNIST_dataset_test)- INITIAL_TESTING_SAMPLES)\n",
    "'''\n",
    "\n",
    "X_test = MNIST_dataset_test.data.numpy().astype(\"float32\") / 255\n",
    "y_test = MNIST_dataset_test.targets.numpy().astype(\"float32\")\n",
    "\n",
    "print(\"TESTING DATA INFORMATION\")\n",
    "print(f\"Image shape: {X_test.shape}\")\n",
    "print(f\"Total test samples: {len(X_test)}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.817718Z",
     "start_time": "2025-02-25T12:56:28.813326Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNIST_Distance_Dataset_Triplet_Loss(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for triplet loss learning on MNIST-like datasets.\n",
    "    \n",
    "    Generates triplets (anchor, positive, negative) for metric learning,\n",
    "    where positive samples share the same class as anchor, while negative\n",
    "    samples belong to different classes.\n",
    "    \n",
    "    Args:\n",
    "        data: Image data array\n",
    "        target: Label array\n",
    "    \"\"\"\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.kers = np.ones((self.data.shape[0], self.data.shape[0]))\n",
    "        self.output_transform = transforms.ToTensor()\n",
    "        self.len = len(self.target)\n",
    "        self.classes = {i:(np.where(self.target == i)[0], np.where(self.target != i)[0] ) for i in range(10)}\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a triplet: (anchor, positive, negative).\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the anchor sample\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (anchor, positive, negative) samples\n",
    "        \"\"\"\n",
    "        return self.get_anchor(idx), self.get_positive(idx), self.get_negative(idx)\n",
    "    \n",
    "    def get_anchor(self, idx):\n",
    "        \"\"\"Returns the anchor sample.\"\"\"\n",
    "        return self.output_transform(self.data[idx])\n",
    "    \n",
    "\n",
    "    def get_positive(self, idx):\n",
    "        \"\"\"Returns a positive sample (same class as anchor).\"\"\"\n",
    "        i = np.random.choice(self.classes[self.target[idx]][0])\n",
    "        return self.output_transform(self.data[i])\n",
    "    \n",
    "    def get_negative(self, idx):\n",
    "        \"\"\"Returns a negative sample (different class from anchor).\"\"\"\n",
    "        i = np.random.choice(self.classes[self.target[idx]][1])\n",
    "        return self.output_transform(self.data[i])\n",
    "\n",
    "\n",
    "    def get_order(self):\n",
    "        \"\"\"Returns the sorting order of samples by label.\"\"\"\n",
    "        return self.target.argsort()\n",
    "\n",
    "\n",
    "    def ordered_pairwise(self):\n",
    "        \"\"\"Returns ordered pairwise kernel matrix.\"\"\"\n",
    "        return self.kers[:,self.get_order()][self.get_order(),:]\n",
    "\n",
    "\n",
    "    def get_flatten(self):\n",
    "        \"\"\"Returns flattened image data.\"\"\"\n",
    "        return self.data.reshape((self.data.shape[0], self.data.shape[1]**2))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:28.973983Z",
     "start_time": "2025-02-25T12:56:28.842175Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "t = MNIST_Distance_Dataset_Triplet_Loss(X_train, y_train)\n",
    "v = MNIST_Distance_Dataset_Triplet_Loss(X_val, y_val)\n",
    "\n",
    "print(f\"Training set size: {len(t)}\")\n",
    "print(f\"Validation set size: {len(v)}\")\n",
    "\n",
    "anchor, pos, neg = t[0]\n",
    "\n",
    "plt.matshow(anchor.squeeze(0))\n",
    "plt.matshow(pos.squeeze(0))\n",
    "plt.matshow(neg.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:29.874119Z",
     "start_time": "2025-02-25T12:56:29.004013Z"
    }
   },
   "outputs": [],
   "source": [
    "reduction_model = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "\n",
    "vis_x = reduction_model.fit_transform(t.get_flatten(), t.target)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Data visualization using TSNE\", weight=\"bold\")\n",
    "ax.scatter(vis_x[:,0], vis_x[:,1], c=t.target, cmap='Dark2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODERS\n",
    "\n",
    "Definition of encoding strategies for data representation in quantum circuits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_encoding(n_features, param_name):\n",
    "    \"\"\"\n",
    "    Amplitude encoding: requires n = 2^k features.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features to encode\n",
    "        param_name: Name for the parameter vector\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Amplitude encoding circuit\n",
    "    \"\"\"\n",
    "    qc = RawFeatureVector(n_features)\n",
    "    qc = qc.assign_parameters(ParameterVector(param_name, n_features))\n",
    "    qc.name = f\"Amplitude Encoding {param_name}\"\n",
    "    return qc\n",
    "\n",
    "amplitude_encoding(4, \"a\").draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YZ angles encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yz_angles_encoding(n_features, param_name=\"x\", params=None):\n",
    "    \"\"\"\n",
    "    YZ angles encoding using RY and RZ rotations.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features to encode\n",
    "        param_name: Parameter vector name (creates new ParameterVector)\n",
    "        params: Existing ParameterVector to reuse\n",
    "    \n",
    "    Note: Specify only one of param_name or params\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: YZ encoding circuit\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = ParameterVector(param_name, n_features)\n",
    "    \n",
    "    n_qubit = math.ceil(n_features / 2)\n",
    "    qc = QuantumCircuit(n_qubit, name=f\"Angles Encoding\")\n",
    "    gates = [qc.ry, qc.rz]\n",
    "\n",
    "    for i in range(n_qubit):\n",
    "        for gate_i in range(2):\n",
    "            pindex = i*2 + gate_i\n",
    "            if pindex < n_features:\n",
    "                gates[gate_i](params[pindex], i)\n",
    "\n",
    "    return qc\n",
    "\n",
    "yz_angles_encoding(8, param_name=\"x\").draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:29.992265Z",
     "start_time": "2025-02-25T12:56:29.919089Z"
    }
   },
   "outputs": [],
   "source": [
    "def pooling_layer(in_lane, param_prefix=\"pool\"):\n",
    "    \"\"\"\n",
    "    Pooling layer for quantum circuit dimensionality reduction.\n",
    "    \n",
    "    Args:\n",
    "        in_lane: Number of input qubits (must be even)\n",
    "        param_prefix: Prefix for parametrized gates\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Pooling layer circuit\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(in_lane, name=\"Pooling Layer\")\n",
    "    params = ParameterVector(param_prefix, length=in_lane //2 *3)\n",
    "\n",
    "    for i in range(in_lane//2):\n",
    "        current = i\n",
    "        aux = i+ in_lane//2\n",
    "\n",
    "        base_param =  current*(in_lane//2 -1)\n",
    "\n",
    "        qc.rz(-np.pi/2, aux)\n",
    "        qc.cx(aux, current)\n",
    "        qc.rz(params[base_param + 0], current)\n",
    "        qc.ry(params[base_param + 1], aux)\n",
    "        qc.cx(current, aux)\n",
    "        qc.ry(params[base_param + 2], aux)\n",
    "\n",
    "    return qc\n",
    "\n",
    "pooling_layer(4, param_prefix=\"m\").draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODING VARIATIONS\n",
    "\n",
    "Alternative encoding strategies and variations for advanced experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HRyRx encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:30.053492Z",
     "start_time": "2025-02-25T12:56:30.006800Z"
    }
   },
   "outputs": [],
   "source": [
    "def hRyRx_encoding(n_features):\n",
    "    \"\"\"\n",
    "    Hadamard-RY-RX encoding strategy.\n",
    "    \n",
    "    Applies Hadamard gate followed by RY and RX rotations on each qubit.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features to encode (must be even)\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: HRyRx encoding circuit\n",
    "    \"\"\"\n",
    "    # Number of qubits required\n",
    "    n_qubits = math.floor(n_features /2) + (1 if (n_features % 2) != 0 else 0)\n",
    "    # Feature vector from neural network\n",
    "    n_feature = n_features\n",
    "    feature_map = QuantumCircuit(n_qubits)\n",
    "    input_params = ParameterVector(name='x', length=n_feature)\n",
    "    idx = 0\n",
    "    for i in range(n_qubits):\n",
    "        feature_map.h(i)\n",
    "        feature_map.ry(input_params[idx], i)\n",
    "        feature_map.rx(input_params[idx+1], i)\n",
    "        idx +=2\n",
    "\n",
    "    return feature_map\n",
    "\n",
    "hRyRx_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HRyRz encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:30.119520Z",
     "start_time": "2025-02-25T12:56:30.076547Z"
    }
   },
   "outputs": [],
   "source": [
    "def hRyRz_encoding(n_features):\n",
    "    \"\"\"\n",
    "    Hadamard-RY-RZ encoding strategy.\n",
    "    \n",
    "    Applies Hadamard gate followed by RY and RZ rotations on each qubit.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features to encode (must be even)\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: HRyRz encoding circuit\n",
    "    \"\"\"\n",
    "    # Number of qubits required\n",
    "    n_qubits = math.floor(n_features /2) + (1 if (n_features % 2) != 0 else 0)\n",
    "    # Feature vector from neural network\n",
    "    n_feature = n_features\n",
    "    feature_map = QuantumCircuit(n_qubits)\n",
    "    input_params = ParameterVector(name='x', length=n_feature)\n",
    "    idx = 0\n",
    "    for i in range(n_qubits):\n",
    "        feature_map.h(i)\n",
    "        feature_map.ry(input_params[idx], i)\n",
    "        feature_map.rz(input_params[idx+1], i)\n",
    "        idx +=2\n",
    "    return feature_map\n",
    "\n",
    "hRyRz_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X angles encoding\n",
    "\n",
    "Feature encoding through RX rotations applied to each qubit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:30.274547Z",
     "start_time": "2025-02-25T12:56:30.244645Z"
    }
   },
   "outputs": [],
   "source": [
    "def x_angles_encoding(n_features, param_name):\n",
    "    \"\"\"\n",
    "    X angles encoding using RX rotations.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features to encode\n",
    "        param_name: Name for the parameter vector\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: RX encoding circuit\n",
    "    \"\"\"\n",
    "    params = ParameterVector(param_name, n_features)\n",
    "    n_qubit = n_features\n",
    "    qc = QuantumCircuit(n_qubit, name=f\"Angles Encoding {param_name}\")\n",
    "    \n",
    "\n",
    "    for i in range(n_qubit):\n",
    "        qc.rx(params[i], i)\n",
    "\n",
    "    return qc\n",
    "\n",
    "x_angles_encoding(4, \"a\").draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RxRy encoding\n",
    "\n",
    "Feature encoding through alternating RX and RY rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RxRy_encoding(n_features, params=None):\n",
    "    \"\"\"\n",
    "    RxRy encoding using RX and RY rotations.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features to encode\n",
    "        params: ParameterVector to use (if None, creates new with name 'x')\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: RxRy encoding circuit\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = ParameterVector(name='x', length=n_features)\n",
    "    \n",
    "    n_qubits = math.ceil(n_features / 2)\n",
    "    feature_map = QuantumCircuit(n_qubits)\n",
    "    idx = 0\n",
    "    \n",
    "    for i in range(n_qubits):\n",
    "        if idx < n_features:\n",
    "            feature_map.rx(params[idx], i)\n",
    "        if idx + 1 < n_features:\n",
    "            feature_map.ry(params[idx + 1], i)\n",
    "        idx += 2\n",
    "    \n",
    "    return feature_map\n",
    "\n",
    "RxRy_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RxRz encoding\n",
    "\n",
    "Feature encoding through alternating RX and RZ rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RxRz_encoding(n_features, params=None):\n",
    "    \"\"\"\n",
    "    RxRz encoding using RX and RZ rotations.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features to encode\n",
    "        params: ParameterVector to use (if None, creates new with name 'x')\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: RxRz encoding circuit\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = ParameterVector(name='x', length=n_features)\n",
    "    \n",
    "    n_qubits = math.ceil(n_features / 2)\n",
    "    feature_map = QuantumCircuit(n_qubits)\n",
    "    idx = 0\n",
    "    \n",
    "    for i in range(n_qubits):\n",
    "        if idx < n_features:\n",
    "            feature_map.rx(params[idx], i)\n",
    "        if idx + 1 < n_features:\n",
    "            feature_map.rz(params[idx + 1], i)\n",
    "        idx += 2\n",
    "    \n",
    "    return feature_map\n",
    "\n",
    "RxRz_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RyRx encoding\n",
    "\n",
    "Feature encoding through alternating RY and RX rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RyRx_encoding(n_features, params=None):\n",
    "    \"\"\"\n",
    "    RyRx encoding using RY and RX rotations.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features to encode\n",
    "        params: ParameterVector to use (if None, creates new with name 'x')\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: RyRx encoding circuit\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = ParameterVector(name='x', length=n_features)\n",
    "    \n",
    "    n_qubits = math.ceil(n_features / 2)\n",
    "    feature_map = QuantumCircuit(n_qubits)\n",
    "    idx = 0\n",
    "    \n",
    "    for i in range(n_qubits):\n",
    "        if idx < n_features:\n",
    "            feature_map.ry(params[idx], i)\n",
    "        if idx + 1 < n_features:\n",
    "            feature_map.rx(params[idx + 1], i)\n",
    "        idx += 2\n",
    "    \n",
    "    return feature_map\n",
    "\n",
    "RyRx_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RzRx encoding\n",
    "\n",
    "Feature encoding through alternating RZ and RX rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RzRx_encoding(n_features, params=None):\n",
    "    \"\"\"\n",
    "    RzRx encoding using RZ and RX rotations.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features to encode\n",
    "        params: ParameterVector to use (if None, creates new with name 'x')\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: RzRx encoding circuit\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = ParameterVector(name='x', length=n_features)\n",
    "    \n",
    "    n_qubits = math.ceil(n_features / 2)\n",
    "    feature_map = QuantumCircuit(n_qubits)\n",
    "    idx = 0\n",
    "    \n",
    "    for i in range(n_qubits):\n",
    "        if idx < n_features:\n",
    "            feature_map.rz(params[idx], i)\n",
    "        if idx + 1 < n_features:\n",
    "            feature_map.rx(params[idx + 1], i)\n",
    "        idx += 2\n",
    "    \n",
    "    return feature_map\n",
    "\n",
    "RzRx_encoding(8).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear full entanglement encoding\n",
    "\n",
    "Feature map with nonlinearity and full entanglement structure among qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map_nonlinear_full_entaglement(num_features, params):\n",
    "    \"\"\"\n",
    "    Feature map with nonlinearity (quadratic) and full entanglement.\n",
    "    \n",
    "    Args:\n",
    "        num_features: Number of features to encode\n",
    "        params: Parameter values for encoding\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Nonlinear fully entangled feature map\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(num_features)\n",
    "    for i in range(num_features):\n",
    "        qc.ry(params[i] ** 2, i)\n",
    "    for i in range(num_features):\n",
    "        for j in range(i + 1, num_features):\n",
    "            qc.cz(i, j)\n",
    "    return qc\n",
    "\n",
    "feature_map_nonlinear_full_entaglement(4, [0.1, 0.2, 0.3, 0.4]).draw(output='mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Reuploading Encoding\n",
    "\n",
    "Standard encoding scheme with cyclic reuploading of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_reuploading_encoding(n_features, param_name=\"x\", layers=3):\n",
    "    \"\"\"\n",
    "    Standard Data Reuploading: data is loaded multiple times into the circuit\n",
    "    in successive layers with different encodings and entanglement.\n",
    "    Only encoding and CNOT gates, no ansatz.\n",
    "    \n",
    "    Structure (repeating cycle):\n",
    "    - Layer 0, 3, 6, ...: yz_angles_encoding (RY, RZ)\n",
    "    - Layer 1, 4, 7, ...: RxRy_encoding (RX, RY)\n",
    "    - Layer 2, 5, 8, ...: RzRx_encoding (RZ, RX)\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of input features\n",
    "        param_name: Parameter name\n",
    "        layers: Number of reuploading layers (default=3)\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Standard reuploading circuit\n",
    "    \"\"\"\n",
    "    n_qubits = math.ceil(n_features / 2)\n",
    "    qc = QuantumCircuit(n_qubits, name=f\"Standard Reuploading\")\n",
    "    params = ParameterVector(param_name, n_features)\n",
    "    \n",
    "    for layer in range(layers):\n",
    "        # Encoder sequence repeats cyclically\n",
    "        encoder_choice = layer % 3\n",
    "        \n",
    "        if encoder_choice == 0:\n",
    "            # YZ encoding (RY, RZ)\n",
    "            encoder_block = yz_angles_encoding(n_features, params=params)\n",
    "                \n",
    "        elif encoder_choice == 1:\n",
    "            # RX-RY encoding (RX, RY)\n",
    "            encoder_block = RxRy_encoding(n_features, params=params)\n",
    "                \n",
    "        else:\n",
    "            # RZ-RX encoding (RZ, RX)\n",
    "            encoder_block = RzRx_encoding(n_features, params=params)\n",
    "\n",
    "        qc.compose(encoder_block, inplace=True)\n",
    "        qc.barrier()\n",
    "        \n",
    "        # Entanglement layer with CNOT gates\n",
    "        for i in range(n_qubits - 1):\n",
    "            qc.cx(i, i + 1)\n",
    "        \n",
    "        # Circular entanglement\n",
    "        if n_qubits > 2:\n",
    "            qc.cx(n_qubits - 1, 0)\n",
    "        \n",
    "        qc.barrier()\n",
    "        qc.barrier()\n",
    "    \n",
    "    return qc\n",
    "\n",
    "# Circuit test\n",
    "standard_reuploading_encoding(8, param_name=\"x\", layers=2).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Reuploading Encoding\n",
    "\n",
    "Encoding scheme with partial reuploading and sliding window mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_reuploading_encoding(n_features, layers=3, features_per_layer=4):\n",
    "    \"\"\"\n",
    "    Partial Data Reuploading with sequential sliding window and cyclic wrapping.\n",
    "    Each layer loads DIFFERENT features on overlapping qubits sequentially.\n",
    "    The feature cycle is determined by layers * features_per_layer and restarts.\n",
    "    Only encoding (RY, RZ) and entanglement (CNOT), no ansatz.\n",
    "    \n",
    "    Example with 3 layers, 4 features/layer:\n",
    "    - Layer 0: features 0-3  → qubits (0,1)\n",
    "    - Layer 1: features 4-7  → qubits (1,2)  \n",
    "    - Layer 2: features 0-3  → qubits (2,3)  [restarts from beginning]\n",
    "    Total cycle: 8 features, independent of n_features\n",
    "    \n",
    "    Args:\n",
    "        n_features: TOTAL number of available features (must be >= features_per_layer)\n",
    "        layers: Number of encoding blocks\n",
    "        features_per_layer: How many features to load in each layer (must be even)\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Partial reuploading sequential with wrapping\n",
    "    \"\"\"\n",
    "    # Ensure features_per_layer is even\n",
    "    if features_per_layer % 2 != 0:\n",
    "        features_per_layer += 1\n",
    "        print(f\"Adjusted features_per_layer to {features_per_layer} for pair encoding.\")\n",
    "    \n",
    "    # Calculate total feature cycle (independent of n_features)\n",
    "    total_cycle = layers * features_per_layer\n",
    "    \n",
    "    # Ensure n_features is sufficient for one cycle iteration\n",
    "    if n_features < features_per_layer:\n",
    "        raise ValueError(f\"n_features ({n_features}) must be >= features_per_layer ({features_per_layer})\")\n",
    "    \n",
    "    # Calculate number of qubits needed for sliding window\n",
    "    n_qubits_per_encoding = math.ceil(features_per_layer / 2)\n",
    "    n_qubits = n_qubits_per_encoding + (layers - 1)\n",
    "    \n",
    "    # Initialize circuit with cycle independent of n_features\n",
    "    qc = QuantumCircuit(n_qubits, name=\"Partial Reuploading Sequential\")\n",
    "    input_params = ParameterVector(\"x\", total_cycle)\n",
    "    \n",
    "    for layer in range(layers):\n",
    "        # Calculate qubit offset for this layer (sliding window)\n",
    "        qubit_offset = layer\n",
    "        \n",
    "        # Calculate starting feature index for this layer (cycles over total_cycle)\n",
    "        start_feature_idx = (layer * features_per_layer) % total_cycle\n",
    "        \n",
    "        # Load feature block onto current window qubits with cyclic wrapping\n",
    "        feature_idx = 0\n",
    "        for i in range(n_qubits_per_encoding):\n",
    "            qubit_target = qubit_offset + i\n",
    "            \n",
    "            # Calculate feature indices with cyclic wrapping over total_cycle\n",
    "            p_idx_1 = (start_feature_idx + feature_idx) % total_cycle\n",
    "            p_idx_2 = (start_feature_idx + feature_idx + 1) % total_cycle\n",
    "            \n",
    "            # Apply encoding gates\n",
    "            qc.ry(input_params[p_idx_1], qubit_target)\n",
    "            qc.rz(input_params[p_idx_2], qubit_target)\n",
    "            \n",
    "            feature_idx += 2\n",
    "\n",
    "        qc.barrier()\n",
    "        \n",
    "        # Entanglement layer on qubits in this layer\n",
    "        qubits_in_layer = list(range(qubit_offset, qubit_offset + n_qubits_per_encoding))\n",
    "        for i in range(len(qubits_in_layer) - 1):\n",
    "            qc.cx(qubits_in_layer[i], qubits_in_layer[i + 1])\n",
    "        \n",
    "        # Circular entanglement (optional, only if more than 2 qubits)\n",
    "        if len(qubits_in_layer) > 2:\n",
    "            qc.cx(qubits_in_layer[-1], qubits_in_layer[0])\n",
    "        \n",
    "        qc.barrier()\n",
    "        qc.barrier()\n",
    "    \n",
    "    return qc\n",
    "\n",
    "# Circuit test - 8 feature cycle (2 layers * 4 features/layer), same with n_features=24, 12, 8, etc.\n",
    "partial_reuploading_encoding(n_features=12, layers=3, features_per_layer=4).draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSATZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:30.360308Z",
     "start_time": "2025-02-25T12:56:30.357268Z"
    }
   },
   "outputs": [],
   "source": [
    "def MPS(num_qubits,parameter_prefix=\"x\", **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a Matrix Product State (MPS) quantum circuit.\n",
    "\n",
    "    Args:\n",
    "        num_qubits (int): The number of qubits in the circuit.\n",
    "        **kwargs: Additional keyword arguments to be passed to the \n",
    "        RealAmplitudes.\n",
    "\n",
    "    Returns:\n",
    "        QuantumCircuit: The constructed MPS quantum circuit.\n",
    "        \n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qubits = range(num_qubits)\n",
    "    for i, j in zip(qubits[:-1], qubits[1:]):\n",
    "        block = RealAmplitudes(2, parameter_prefix=f\"{parameter_prefix}_{i},{j}\", **kwargs)\n",
    "        qc.compose(block, [i, j], inplace=True)\n",
    "        # if i < num_qubits - 2:\n",
    "        #     qc.barrier()\n",
    "    return qc\n",
    "\n",
    "MPS(4).draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwoLocalMPS(num_qubits,parameter_prefix=\"θ\", **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a Matrix Product State (MPS) quantum circuit using TwoLocal.\n",
    "\n",
    "    Args:\n",
    "        num_qubits (int): The number of qubits in the circuit.\n",
    "        **kwargs: Additional keyword arguments to be passed to TwoLocal.\n",
    "\n",
    "    Returns:\n",
    "        QuantumCircuit: The constructed MPS quantum circuit.\n",
    "        \n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qubits = range(num_qubits)\n",
    "    for i, j in zip(qubits[:-1], qubits[1:]):\n",
    "        block = TwoLocal(\n",
    "            2,\n",
    "            rotation_blocks='rx',\n",
    "            entanglement='reverse_linear',\n",
    "            entanglement_blocks='cx',\n",
    "            parameter_prefix=f'{parameter_prefix}_{i},{j}',\n",
    "            insert_barriers=True,\n",
    "            **kwargs\n",
    "\n",
    "        )\n",
    "        qc.compose(block, [i, j], inplace=True)\n",
    "    return qc\n",
    "\n",
    "TwoLocalMPS(4).draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ansatz(num_qubits, parameter_prefix=\"θ\", ansatz_type=\"MPS\", **kwargs):\n",
    "    \"\"\"\n",
    "    Crea un ansatz del tipo specificato.\n",
    "    \n",
    "    Args:\n",
    "        num_qubits (int): Numero di qubit.\n",
    "        parameter_prefix (str): Prefisso per i parametri.\n",
    "        ansatz_type (str): Tipo di ansatz - \"MPS\" (RealAmplitudes) o \"TwoLocalMPS\" (TwoLocal).\n",
    "        **kwargs: Argomenti aggiuntivi passati alla funzione ansatz.\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Il circuito ansatz.\n",
    "    \"\"\"\n",
    "    if ansatz_type == \"MPS\":\n",
    "        return MPS(num_qubits, parameter_prefix=parameter_prefix, **kwargs)\n",
    "    elif ansatz_type == \"TwoLocalMPS\":\n",
    "        return TwoLocalMPS(num_qubits, parameter_prefix=parameter_prefix, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Ansatz type '{ansatz_type}' non supportato. Usa 'MPS' o 'TwoLocalMPS'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIRCUITS\n",
    "\n",
    "Definizione e composizione dei circuiti quantistici per l'apprendimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original circuit definition (with RealAmplitudes and Pooling)\n",
    "\n",
    "Uncomment the necessary encoding to perform specific experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = yz_angles_encoding(8, param_name=\"e\")\n",
    "#encoding = hRyRx_encoding(8)\n",
    "#encoding= hRyRz_encoding(8)\n",
    "#encoding = RyRx_encoding(8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ansatz = QuantumCircuit(4)\n",
    "ansatz.barrier()\n",
    "############################2#######################\n",
    "ansatz = ansatz.compose(RealAmplitudes(num_qubits=4, reps=1, name=\"Layer1\", parameter_prefix=\"l1\"))\n",
    "ansatz.barrier()\n",
    "############################3#######################\n",
    "ansatz = ansatz.compose(pooling_layer(4, \"pool2\"))\n",
    "ansatz.barrier()\n",
    "############################4#######################\n",
    "ansatz = ansatz.compose(RealAmplitudes(num_qubits=2, reps=1, name=\"Layer2\",parameter_prefix=\"l2\"), qubits=[2,3])\n",
    "ansatz.barrier()\n",
    "\n",
    "ansatz.decompose().draw(output=\"mpl\")\n",
    "\n",
    "\n",
    "qnn = QuantumCircuit(4).compose(encoding).compose(ansatz)\n",
    "\n",
    "display(qnn.decompose().draw(\"mpl\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Network circuit (MPS only)\n",
    "\n",
    "Example of quantum circuit with pure tensor network (MPS) structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = yz_angles_encoding(8, param_name=\"e\")\n",
    "\n",
    "\n",
    "qnn = QuantumCircuit(4).compose(encoding)\n",
    "\n",
    "# Adding MPS to the circuit\n",
    "ansatz = MPS(num_qubits=4, parameter_prefix=\"mps\")\n",
    "qnn = qnn.compose(ansatz)\n",
    "\n",
    "# Circuit visualization\n",
    "display(qnn.draw(\"mpl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parameter counts for encoding, ansatz, and total circuit\n",
    "print(\"Encoding parameters:\", len(encoding.parameters))\n",
    "print(\"Ansatz parameters:\", len(ansatz.parameters))\n",
    "print(\"Total circuit parameters:\", len(qnn.parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard reuploading circuit\n",
    "\n",
    "Quantum circuit with standard data reuploading for metric learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "import math\n",
    "\n",
    "def build_mps_reuploading_model(n_features, layers=2, fixed_encoder=None, ansatz_type=\"MPS\"):\n",
    "    \"\"\"\n",
    "    Constructs a reuploading classifier using combinations of encoders and ansatz.\n",
    "    \n",
    "    Args:\n",
    "        n_features (int): Number of input features.\n",
    "        layers (int): Number of [Encoder -> Ansatz] sequence repetitions (default=2).\n",
    "        ansatz_type (str): Type of ansatz - \"MPS\" or \"TwoLocalMPS\" (default=\"MPS\").\n",
    "        fixed_encoder (str): If None, cycles between three encoders (yz, RxRy, RzRx).\n",
    "                            If specified, uses the same encoder for all layers.\n",
    "                            Options: \"yz\", \"RxRy\", \"RzRx\", \"RxRz\", \"RyRx\" (default=None).\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Complete reuploading quantum circuit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate required number of qubits\n",
    "    n_qubits = math.ceil(n_features / 2)\n",
    "    \n",
    "    # Initialize final circuit\n",
    "    full_qc = QuantumCircuit(n_qubits, name=f\"{ansatz_type} Re-uploading Classifier\")\n",
    "    \n",
    "    # Create input parameters ONCE (shared across all layers)\n",
    "    input_params = ParameterVector(\"x\", n_features)\n",
    "    \n",
    "    # Reuploading cycle (Sandwich architecture)\n",
    "    for layer in range(layers):\n",
    "        \n",
    "        # --- A. Add Encoder ---\n",
    "        if fixed_encoder is None:\n",
    "            # Cyclic behavior: cycles between three encoders\n",
    "            encoder_choice = layer % 3\n",
    "            \n",
    "            if encoder_choice == 0:\n",
    "                # YZ encoding (RY, RZ)\n",
    "                encoder_block = yz_angles_encoding(n_features, params=input_params)\n",
    "            elif encoder_choice == 1:\n",
    "                # RX-RY encoding (RX, RY)\n",
    "                encoder_block = RxRy_encoding(n_features, params=input_params)\n",
    "            else:\n",
    "                # RZ-RX encoding (RZ, RX)\n",
    "                encoder_block = RzRx_encoding(n_features, params=input_params)\n",
    "        else:\n",
    "            # Use specified fixed encoder for all layers\n",
    "            if fixed_encoder == \"yz\":\n",
    "                encoder_block = yz_angles_encoding(n_features, params=input_params)\n",
    "            elif fixed_encoder == \"RxRy\":\n",
    "                encoder_block = RxRy_encoding(n_features, params=input_params)\n",
    "            elif fixed_encoder == \"RxRz\":\n",
    "                encoder_block = RxRz_encoding(n_features, params=input_params)\n",
    "            elif fixed_encoder == \"RyRx\":\n",
    "                encoder_block = RyRx_encoding(n_features, params=input_params)\n",
    "            elif fixed_encoder == \"RzRx\":\n",
    "                encoder_block = RzRx_encoding(n_features, params=input_params)\n",
    "            else:\n",
    "                raise ValueError(f\"Encoder '{fixed_encoder}' not supported. \"\n",
    "                               \"Use: 'yz', 'RxRy', 'RxRz', 'RyRx', 'RzRx' or None for cyclic behavior.\")\n",
    "\n",
    "        full_qc.compose(encoder_block, inplace=True)\n",
    "        full_qc.barrier()\n",
    "        \n",
    "        # --- B. Add Ansatz (MPS or TwoLocalMPS) ---\n",
    "        ansatz_block = create_ansatz(n_qubits, parameter_prefix=f\"w_L{layer}\", ansatz_type=ansatz_type, reps=1)\n",
    "        full_qc.compose(ansatz_block, inplace=True)\n",
    "        \n",
    "        full_qc.barrier()\n",
    "        full_qc.barrier()\n",
    "\n",
    "    return full_qc\n",
    "\n",
    "# --- TEST ---\n",
    "# Example with 8 features and 2 depth layers\n",
    "\n",
    "ansatz_type = \"MPS\"  # Choose between \"MPS\" and \"TwoLocalMPS\"\n",
    "fixed_enc = None  # Choose between None (cyclic), \"yz\", \"RxRy\", \"RxRz\", \"RyRx\", \"RzRx\"\n",
    "qnn_std_reupload = build_mps_reuploading_model(n_features=8, layers=2, fixed_encoder=fixed_enc, ansatz_type=ansatz_type)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Circuit with encoder: {fixed_enc}, ansatz: {ansatz_type}\")\n",
    "print(\"Encoding parameters:\", len([p for p in qnn_std_reupload.parameters if p.name.startswith('x')]))\n",
    "print(\"Ansatz parameters:\", len([p for p in qnn_std_reupload.parameters if p.name.startswith('w')]))\n",
    "print(\"Total parameters:\", len(qnn_std_reupload.parameters))\n",
    "qnn_std_reupload.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial reuploading circuit\n",
    "\n",
    "Quantum circuit with partial data reuploading and sliding window mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_partial_mps_model(n_features, layers=3, features_per_layer=4, fixed_encoder=\"yz\", ansatz_type=\"MPS\"):\n",
    "    \"\"\"\n",
    "    Constructs a Sequential Partial classifier with sliding window architecture.\n",
    "    Each layer loads DIFFERENT features on sequentially overlapping qubits.\n",
    "    \n",
    "    Example with 4 qubits, 3 layers, 4 features/layer:\n",
    "    - Layer 0: features 0-3  → qubits (0,1)\n",
    "    - Layer 1: features 4-7  → qubits (1,2)  \n",
    "    - Layer 2: features 8-11 → qubits (2,3)\n",
    "    Total: 12 features mapped\n",
    "    \n",
    "    Args:\n",
    "        n_features (int): TOTAL number of features to map (increased from standard case).\n",
    "        layers (int): Number of [Encoding -> Ansatz] blocks.\n",
    "        features_per_layer (int): How many features to load in each layer (must be even).\n",
    "        ansatz_type (str): Type of ansatz - \"MPS\" or \"TwoLocalMPS\" (default=\"MPS\").\n",
    "        fixed_encoder (str): Encoder to use for all layers.\n",
    "                            Options: \"yz\", \"RxRy\", \"RxRz\", \"RyRx\", \"RzRx\" (default=\"yz\").\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit: Partial reuploading quantum circuit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure features_per_layer is even\n",
    "    if features_per_layer % 2 != 0:\n",
    "        features_per_layer += 1\n",
    "    \n",
    "    # Encoder validation\n",
    "    valid_encoders = {\"yz\", \"RxRy\", \"RxRz\", \"RyRx\", \"RzRx\"}\n",
    "    if fixed_encoder not in valid_encoders:\n",
    "        raise ValueError(f\"Encoder '{fixed_encoder}' not supported. \"\n",
    "                        f\"Use: {valid_encoders}\")\n",
    "    \n",
    "    # Map encoder to gate pairs (gate1, gate2)\n",
    "    encoder_gates = {\n",
    "        \"yz\": (\"ry\", \"rz\"),\n",
    "        \"RxRy\": (\"rx\", \"ry\"),\n",
    "        \"RxRz\": (\"rx\", \"rz\"),\n",
    "        \"RyRx\": (\"ry\", \"rx\"),\n",
    "        \"RzRx\": (\"rz\", \"rx\"),\n",
    "    }\n",
    "    gate1_name, gate2_name = encoder_gates[fixed_encoder]\n",
    "    \n",
    "    # Calculate number of qubits needed for sliding window\n",
    "    # First layer uses features_per_layer/2 qubits, then each layer adds 1 qubit\n",
    "    n_qubits_per_encoding = math.ceil(features_per_layer / 2)\n",
    "    n_qubits = n_qubits_per_encoding + (layers - 1)\n",
    "    \n",
    "    # Calculate total number of features that will be mapped\n",
    "    total_features_mapped = layers * features_per_layer\n",
    "    \n",
    "    # Initialize circuit\n",
    "    full_qc = QuantumCircuit(n_qubits, name=f\"Sequential Sliding Window ({fixed_encoder})\")\n",
    "    input_params = ParameterVector(\"x\", total_features_mapped)\n",
    "    \n",
    "    for layer in range(layers):\n",
    "        \n",
    "        # --- A. SEQUENTIAL SLIDING WINDOW ENCODER ---\n",
    "        # Calculate qubit offset for this layer (sliding window)\n",
    "        qubit_offset = layer\n",
    "        \n",
    "        # Calculate starting feature index for this layer (sequential, not cyclic)\n",
    "        start_feature_idx = layer * features_per_layer\n",
    "        \n",
    "        # Load feature block onto current window qubits\n",
    "        feature_idx = 0\n",
    "        for i in range(n_qubits_per_encoding):\n",
    "            qubit_target = qubit_offset + i\n",
    "            \n",
    "            p_idx_1 = start_feature_idx + feature_idx\n",
    "            p_idx_2 = start_feature_idx + feature_idx + 1\n",
    "            \n",
    "            # Apply gates only if index is valid\n",
    "            if p_idx_1 < total_features_mapped:\n",
    "                getattr(full_qc, gate1_name)(input_params[p_idx_1], qubit_target)\n",
    "            if p_idx_2 < total_features_mapped:\n",
    "                getattr(full_qc, gate2_name)(input_params[p_idx_2], qubit_target)\n",
    "            \n",
    "            feature_idx += 2\n",
    "        \n",
    "        full_qc.barrier()\n",
    "        \n",
    "        # --- B. ANSATZ (MPS or TwoLocalMPS) only on populated qubits in this layer ---\n",
    "        # Calculate qubits used in this layer\n",
    "        qubits_in_layer = list(range(qubit_offset, qubit_offset + n_qubits_per_encoding))\n",
    "        ansatz_block = create_ansatz(n_qubits_per_encoding, parameter_prefix=f\"w_L{layer}\", ansatz_type=ansatz_type, reps=1)\n",
    "        full_qc.compose(ansatz_block, qubits_in_layer, inplace=True)\n",
    "        \n",
    "        full_qc.barrier()\n",
    "        full_qc.barrier()\n",
    "    \n",
    "    return full_qc\n",
    "\n",
    "# --- TEST ---\n",
    "# Example with 12 features, 3 layers, and 4 features per layer\n",
    "\n",
    "ansatz_type = \"MPS\"  # Choose between \"MPS\" and \"TwoLocalMPS\"\n",
    "fixed_enc = \"yz\"  # Choose encoder: \"yz\", \"RxRy\", \"RxRz\", \"RyRx\", \"RzRx\"\n",
    "qnn_partial_reupload = build_partial_mps_model(n_features=12, layers=3, features_per_layer=4, \n",
    "                                               fixed_encoder=fixed_enc, ansatz_type=ansatz_type)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Circuit with encoder: {fixed_enc}, ansatz: {ansatz_type}\")\n",
    "print(\"Encoding parameters:\", len([p for p in qnn_partial_reupload.parameters if p.name.startswith('x')]))\n",
    "print(\"Ansatz parameters:\", len([p for p in qnn_partial_reupload.parameters if p.name.startswith('w')]))\n",
    "print(\"Total parameters:\", len(qnn_partial_reupload.parameters))\n",
    "qnn_partial_reupload.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYBRID NETWORKS\n",
    "\n",
    "Hybrid architectures combining classical neural networks and quantum models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-features per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.787845Z",
     "start_time": "2025-02-25T12:56:31.753308Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# QUANTUM MODEL: 8 FEATURES\n",
    "# ===============================\n",
    "\"\"\"\n",
    "This block defines the hybrid model for images with 8 extracted features,\n",
    "combining a classical convolutional neural network and a quantum circuit.\n",
    "The circuit is constructed using the build_mps_reuploading_model function.\n",
    "\"\"\"\n",
    "\n",
    "from qiskit.primitives import Sampler\n",
    "def parity(x):\n",
    "    \"\"\"Returns the parity (number of 1-bits) of an integer x.\"\"\"\n",
    "    return f\"{bin(x)}\".count(\"1\")\n",
    "\n",
    "# Quantum circuit construction for 8 features\n",
    "qnn_circuit_8 = build_mps_reuploading_model(n_features=8, layers=2)\n",
    "\n",
    "# Parameter separation: first 8 are inputs, remaining are ansatz weights\n",
    "input_ps_8 = list(qnn_circuit_8.parameters)[:8]\n",
    "weight_ps_8 = list(qnn_circuit_8.parameters)[8:]\n",
    "\n",
    "qmodel_8 = SamplerQNN(\n",
    "    circuit=qnn_circuit_8,\n",
    "    input_params=input_ps_8,\n",
    "    weight_params=weight_ps_8,\n",
    "    input_gradients=True\n",
    ")\n",
    "\n",
    "class HybridRegressorConvNet8Features(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid neural network combining a classical CNN with a quantum circuit.\n",
    "    \n",
    "    Args:\n",
    "        qm1: Quantum model (SamplerQNN) to connect as a layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, qm1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional blocks for feature extraction\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5,5)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Reduction to 8 features for quantum layer\n",
    "        self.reduction = nn.Sequential(\n",
    "            nn.Linear(in_features=576, out_features=120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=120, out_features=8)\n",
    "        )\n",
    "\n",
    "        # Output layer (from 16 to 16 qubit outputs)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(in_features=16, out_features=16),\n",
    "        )\n",
    "\n",
    "        # Quantum layer connected via TorchConnector\n",
    "        self.q1 = TorchConnector(qm1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Executes forward pass: feature extraction, reduction, quantum layer, and final output.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch of images)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output embeddings\n",
    "        \"\"\"\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        # Pass through quantum layer and normalization\n",
    "        x = self.q1(x) * 100\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Model instantiation and summary\n",
    "distance_model_8 = HybridRegressorConvNet8Features(qmodel_8)\n",
    "distance_model_8 = distance_model_8.to(device)\n",
    "summary(distance_model_8, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12-features per image\n",
    "\n",
    "Architecture and pipeline for images with 12 extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# QUANTUM MODEL: 12 FEATURES\n",
    "# ===============================\n",
    "\"\"\"\n",
    "This block defines the hybrid model for images with 12 extracted features,\n",
    "combining a classical convolutional neural network and a quantum circuit.\n",
    "The circuit is constructed using the build_partial_mps_model function.\n",
    "\"\"\"\n",
    "\n",
    "from qiskit.primitives import Sampler\n",
    "\n",
    "# Quantum circuit construction for 12 features\n",
    "qnn_circuit_12 = build_partial_mps_model(n_features=12, layers=3, features_per_layer=4)\n",
    "\n",
    "# Parameter separation: first 12 are inputs, remaining are ansatz weights\n",
    "input_ps_12 = list(qnn_circuit_12.parameters)[:12]\n",
    "weight_ps_12 = list(qnn_circuit_12.parameters)[12:]\n",
    "\n",
    "qmodel_12 = SamplerQNN(\n",
    "    circuit=qnn_circuit_12,\n",
    "    input_params=input_ps_12,\n",
    "    weight_params=weight_ps_12,\n",
    "    input_gradients=True\n",
    ")\n",
    "\n",
    "class HybridRegressorConvNet12Features(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid neural network combining a classical CNN with a quantum circuit.\n",
    "    \n",
    "    Args:\n",
    "        qm1: Quantum model (SamplerQNN) to connect as a layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, qm1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional blocks for feature extraction\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5,5)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Reduction to 12 features for quantum layer\n",
    "        self.reduction = nn.Sequential(\n",
    "            nn.Linear(in_features=576, out_features=120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=120, out_features=12)\n",
    "        )\n",
    "\n",
    "        # Output layer (from 16 to 16 qubit outputs)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(in_features=16, out_features=16),\n",
    "        )\n",
    "\n",
    "        # Quantum layer connected via TorchConnector\n",
    "        self.q1 = TorchConnector(qm1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Executes forward pass: feature extraction, reduction, quantum layer, and final output.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch of images)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output embeddings\n",
    "        \"\"\"\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        # Pass through quantum layer and normalization\n",
    "        x = self.q1(x) * 100\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Model instantiation and summary\n",
    "distance_model_12 = HybridRegressorConvNet12Features(qmodel_12)\n",
    "distance_model_12 = distance_model_12.to(device)\n",
    "summary(distance_model_12, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING - Model Training Procedures\n",
    "\n",
    "Section dedicated to model training and epoch management with standardized output formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training configuration\n",
    "\n",
    "Initial configuration for model training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.830824Z",
     "start_time": "2025-02-25T12:56:31.827162Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 12\n",
    "if n_features == 8:\n",
    "    model_to_train = qmodel_8\n",
    "    distance_model = HybridRegressorConvNet8Features(model_to_train)\n",
    "else:\n",
    "    model_to_train = qmodel_12\n",
    "    distance_model = HybridRegressorConvNet12Features(model_to_train)\n",
    "training_dataloader = DataLoader(t, batch_size=8, shuffle=True)\n",
    "val_data_loader = DataLoader(v, batch_size=8, shuffle=True)\n",
    "distance_model = distance_model.to(device)\n",
    "optimizer = torch.optim.SGD(distance_model.parameters(), lr=1e-2)\n",
    "loss = torch.nn.TripletMarginLoss(margin=2)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.928352Z",
     "start_time": "2025-02-25T12:56:31.855516Z"
    }
   },
   "outputs": [],
   "source": [
    "model_to_train.circuit.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Main model training loop with standardized output formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.954138Z",
     "start_time": "2025-02-25T12:56:31.950554Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, criterion, train_data_loader, val_data_loader, device, validation_step=False, print_at=1, seed=42):\n",
    "    \"\"\"\n",
    "    Executes model training with standardized output formatting.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model to train\n",
    "        epochs: Number of training epochs\n",
    "        optimizer: Optimization algorithm\n",
    "        criterion: Loss function\n",
    "        train_data_loader: Training data loader\n",
    "        val_data_loader: Validation data loader (optional)\n",
    "        device: Computation device (CPU/GPU)\n",
    "        validation_step: Whether to perform validation (default=False)\n",
    "        print_at: Print frequency (every N epochs, default=1)\n",
    "        seed: Random seed for reproducibility (default=42)\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    set_seed(seed)\n",
    "    for epoch in range(epochs):\n",
    "        set_seed(seed + epoch)\n",
    "        \n",
    "        prnt = (epoch % print_at) != 0 if epoch != (epochs - 1) else False\n",
    "        \n",
    "        if not prnt:\n",
    "            print(\"\\033[1mEpoch {}/{}\\033[0m\".format(epoch + 1, epochs))\n",
    "        \n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_samples = 0\n",
    "        \n",
    "        for anchor, positive, negative in tqdm(train_data_loader, disable=prnt):\n",
    "            optimizer.zero_grad()\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "            e_A = model(anchor)\n",
    "            e_P = model(positive)\n",
    "            e_N = model(negative)\n",
    "            loss = criterion(e_A, e_P, e_N)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_samples += positive.size(0)\n",
    "        train_loss /= len(train_data_loader)\n",
    "        \n",
    "        if not prnt:\n",
    "            print(f\"TRAINING | Loss: {train_loss:2.6f}\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop with early stopping\n",
    "\n",
    "Training loop with early stopping logic and automatic checkpointing mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T12:56:31.978202Z",
     "start_time": "2025-02-25T12:56:31.973369Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, criterion, train_data_loader, val_data_loader, device, \n",
    "          validation_step=False, print_at=1, early_stopping_patience=3, \n",
    "          min_delta=0.001, restore_best_weights=True, seed=42, \n",
    "          checkpoint_interval=100, checkpoint_path=\"model_results/triplet_loss\"):\n",
    "    \"\"\"\n",
    "    Training function with automatic checkpointing every N epochs and standardized output.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model to train\n",
    "        epochs: Number of training epochs\n",
    "        optimizer: Optimization algorithm\n",
    "        criterion: Loss function\n",
    "        train_data_loader: Training data loader\n",
    "        val_data_loader: Validation data loader\n",
    "        device: Computation device (CPU/GPU)\n",
    "        validation_step: Whether to perform validation (default=False)\n",
    "        print_at: Print frequency (default=1)\n",
    "        early_stopping_patience: Patience for early stopping (default=3)\n",
    "        min_delta: Minimum improvement threshold (default=0.001)\n",
    "        restore_best_weights: Whether to restore best weights (default=True)\n",
    "        seed: Random seed (default=42)\n",
    "        checkpoint_interval: Checkpoint save frequency (default=100)\n",
    "        checkpoint_path: Directory for saving checkpoints\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained_model, checkpoint_epochs, checkpoint_logs)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "    # Initialize time tracking\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    set_seed(seed)\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    best_model_state = None\n",
    "    checkpoint_epochs = []\n",
    "    checkpoint_logs = {}  # Dictionary to save time and loss for each checkpoint\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        set_seed(seed + epoch)\n",
    "        prnt = (epoch % print_at) != 0 if epoch != (epochs-1) else False\n",
    "\n",
    "        # Initialize early_stopping_triggered at the beginning of each epoch\n",
    "        early_stopping_triggered = False\n",
    "\n",
    "        if not prnt:\n",
    "            print(f\"\\033[1mEpoch {epoch+1}/{epochs}\\033[0m\")\n",
    "\n",
    "        ### --> Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        for anchor, positive, negative in tqdm(train_data_loader, disable=prnt):\n",
    "            optimizer.zero_grad()\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            e_A = model(anchor)\n",
    "            e_P = model(positive)\n",
    "            e_N = model(negative)\n",
    "\n",
    "            loss = criterion(e_A, e_P, e_N)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_samples += positive.size(0)\n",
    "\n",
    "        train_loss /= len(train_data_loader)\n",
    "        \n",
    "        ### --> Validation Phase (if requested)\n",
    "        val_loss = 0.0\n",
    "        if validation_step and not prnt:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for anchor, positive, negative in val_data_loader:\n",
    "                    anchor = anchor.to(device)\n",
    "                    positive = positive.to(device)\n",
    "                    negative = negative.to(device)\n",
    "\n",
    "                    e_A = model(anchor)\n",
    "                    e_P = model(positive)\n",
    "                    e_N = model(negative)\n",
    "\n",
    "                    loss = criterion(e_A, e_P, e_N)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                val_loss /= len(val_data_loader)\n",
    "\n",
    "        ### --> Early Stopping Logic\n",
    "        if validation_step and not prnt:\n",
    "            # Check for improvement\n",
    "            if (best_loss - val_loss) > min_delta:\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                no_improvement_count = 0\n",
    "                # Save best model state\n",
    "                if restore_best_weights:\n",
    "                    print(f\"Best epoch: {best_epoch+1}\")\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= early_stopping_patience:\n",
    "                    print(f\"\\n{color.RED}Early stopping at epoch {epoch+1}{color.END}\")\n",
    "                    early_stopping_triggered = True\n",
    "\n",
    "        ### --> Print results\n",
    "        if not prnt:\n",
    "            log_str = f\"TRAINING | Loss: {train_loss:2.6f}\"\n",
    "            if validation_step:\n",
    "                log_str += f\" | VALIDATION -> Loss: {val_loss:2.6f}\"\n",
    "            print(log_str)\n",
    "            print(\"\")\n",
    "\n",
    "        ### --> Save checkpoint every checkpoint_interval epochs\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            # Calculate total training time in minutes\n",
    "            elapsed_time_minutes = (time.time() - training_start_time) / 60\n",
    "            \n",
    "            checkpoint_filename = f\"{checkpoint_path}/checkpoint_epoch_{epoch+1}.pt\"\n",
    "            torch.save(model.state_dict(), checkpoint_filename)\n",
    "            checkpoint_epochs.append(epoch + 1)\n",
    "            \n",
    "            # Save time and train loss for this checkpoint\n",
    "            checkpoint_logs[epoch + 1] = {\n",
    "                'training_time_minutes': elapsed_time_minutes,\n",
    "                'train_loss': train_loss\n",
    "            }\n",
    "            \n",
    "            print(f\"{color.GREEN}Checkpoint saved: {checkpoint_filename}{color.END}\")\n",
    "            print(f\"Training time: {elapsed_time_minutes:.2f} minutes\")\n",
    "            print(f\"Train Loss: {train_loss:.6f}\")\n",
    "            print(\"\")\n",
    "\n",
    "        ### --> Stop training if early stopping triggered\n",
    "        if early_stopping_triggered:\n",
    "            break\n",
    "\n",
    "    ### --> Restore best weights if requested\n",
    "    if restore_best_weights and best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"{color.GREEN}Restored best model weights{color.END}\")\n",
    "\n",
    "    return model, checkpoint_epochs, checkpoint_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training execution\n",
    "\n",
    "Training loop execution and checkpoint saving with standardized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T14:14:49.161048Z",
     "start_time": "2025-02-25T12:56:32.035203Z"
    }
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT CONFIGURATION\n",
    "CHECKPOINT_PATH = \"model_results/triplet_loss/Standard_TwoLocalMPS\"\n",
    "CHECKPOINT_INTERVAL = 1  # Save every epoch\n",
    "\n",
    "# Training without Early Stopping\n",
    "distance_model, checkpoint_epochs, checkpoint_logs = train(\n",
    "    model=distance_model,\n",
    "    epochs=2,\n",
    "    optimizer=optimizer,\n",
    "    criterion=loss,\n",
    "    train_data_loader=training_dataloader,\n",
    "    val_data_loader=None,\n",
    "    validation_step=False,\n",
    "    device=device,\n",
    "    print_at=1,\n",
    "    seed=42,\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "    checkpoint_path=CHECKPOINT_PATH\n",
    ")\n",
    "\n",
    "# Save checkpoint logs for evaluation\n",
    "CHECKPOINT_EVAL_LOGS = checkpoint_logs\n",
    "\n",
    "print(f\"\\n{color.BOLD}Training completed{color.END}\")\n",
    "print(f\"Checkpoints saved at epochs: {checkpoint_epochs}\")\n",
    "\n",
    "\n",
    "# Training with Early Stopping (commented out)\n",
    "# distance_model, checkpoint_epochs, checkpoint_logs = train(\n",
    "#     distance_model,\n",
    "#     epochs=200,\n",
    "#     optimizer=optimizer,\n",
    "#     criterion=loss,\n",
    "#     train_data_loader=training_dataloader,\n",
    "#     val_data_loader=val_data_loader,\n",
    "#     device=device,\n",
    "#     validation_step=True,\n",
    "#     early_stopping_patience=4,\n",
    "#     min_delta=0.005,\n",
    "#     seed=42,\n",
    "#     checkpoint_interval=CHECKPOINT_INTERVAL,\n",
    "#     checkpoint_path=CHECKPOINT_PATH\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOMATIC CHECKPOINT EVALUATION ON TEST SET\n",
    "\n",
    "Automatic performance evaluation of saved checkpoints on the test set with standardized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def evaluate_checkpoint_on_test(checkpoint_path, model_template, test_dataset, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint and evaluates performance on the test set with standardized output.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        model_template: Model template for loading weights\n",
    "        test_dataset: Test dataset\n",
    "        device: Computation device\n",
    "        batch_size: Batch size for evaluation (default=128)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation results dictionary\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    model_template.load_state_dict(torch.load(checkpoint_path))\n",
    "    model_template.eval()\n",
    "    \n",
    "    # Generate embeddings for test set\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_embedding_data = np.empty((0, 16))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for anchor, _, _ in tqdm(test_loader, desc=f\"Generating embeddings\"):\n",
    "            anchor = anchor.to(device)\n",
    "            embeddings = model_template(anchor).to(\"cpu\").numpy()\n",
    "            test_embedding_data = np.concatenate((test_embedding_data, embeddings), axis=0)\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=10, linkage=\"average\")\n",
    "    agg_prediction = agg_clustering.fit_predict(test_embedding_data)\n",
    "    agg_silhouette, agg_purity = evaluate_clustering_table(test_embedding_data, agg_prediction, test_dataset.target)\n",
    "    \n",
    "    # KMeans Clustering\n",
    "    kmeans_clustering = KMeans(n_clusters=10, init=\"k-means++\", n_init=10, random_state=42)\n",
    "    kmeans_prediction = kmeans_clustering.fit_predict(test_embedding_data)\n",
    "    kmeans_silhouette, kmeans_purity = evaluate_clustering_table(test_embedding_data, kmeans_prediction, test_dataset.target)\n",
    "    \n",
    "    return {\n",
    "        'checkpoint': checkpoint_path,\n",
    "        'test_embedding': test_embedding_data,\n",
    "        'agg_silhouette': agg_silhouette,\n",
    "        'agg_purity': agg_purity,\n",
    "        'agg_prediction': agg_prediction,\n",
    "        'kmeans_silhouette': kmeans_silhouette,\n",
    "        'kmeans_purity': kmeans_purity,\n",
    "        'kmeans_prediction': kmeans_prediction\n",
    "    }\n",
    "\n",
    "def evaluate_all_checkpoints(checkpoint_path, model_template, test_dataset, device):\n",
    "    \"\"\"\n",
    "    Evaluates all saved checkpoints from training with standardized output.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Directory containing checkpoints\n",
    "        model_template: Model template for loading weights\n",
    "        test_dataset: Test dataset\n",
    "        device: Computation device\n",
    "    \n",
    "    Returns:\n",
    "        list: List of evaluation results for all checkpoints\n",
    "    \"\"\"\n",
    "    # Find all checkpoints\n",
    "    checkpoint_files = sorted(glob.glob(f\"{checkpoint_path}/checkpoint_epoch_*.pt\"))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(f\"{color.RED}No checkpoints found in {checkpoint_path}{color.END}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n{color.BOLD}=== CHECKPOINT EVALUATION ON TEST SET ==={color.END}\")\n",
    "    print(f\"Found {len(checkpoint_files)} checkpoints to evaluate\\n\")\n",
    "    \n",
    "    results = []\n",
    "    for checkpoint_file in checkpoint_files:\n",
    "        # Extract epoch number from filename\n",
    "        epoch_num = int(checkpoint_file.split('_')[-1].replace('.pt', ''))\n",
    "        print(f\"\\n{color.CYAN}{'='*60}{color.END}\")\n",
    "        print(f\"{color.BOLD}Checkpoint Epoch {epoch_num}{color.END}\")\n",
    "        print(f\"{color.CYAN}{'='*60}{color.END}\\n\")\n",
    "        \n",
    "        result = evaluate_checkpoint_on_test(checkpoint_file, model_template, test_dataset, device)\n",
    "        result['epoch'] = epoch_num\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print results with standardized formatting\n",
    "        print(f\"\\n{color.GREEN}Test Set Results (10,000 samples):{color.END}\")\n",
    "        print(f\"  Agglomerative Clustering:\")\n",
    "        print(f\"    - Silhouette: {result['agg_silhouette']:.4f}\")\n",
    "        print(f\"    - Purity:     {result['agg_purity']:.4f}\")\n",
    "        print(f\"  KMeans Clustering:\")\n",
    "        print(f\"    - Silhouette: {result['kmeans_silhouette']:.4f}\")\n",
    "        print(f\"    - Purity:     {result['kmeans_purity']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare test dataset\n",
    "t_test = MNIST_Distance_Dataset_Triplet_Loss(X_test, y_test)\n",
    "\n",
    "# Evaluate all checkpoints\n",
    "checkpoint_results = evaluate_all_checkpoints(\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    model_template=distance_model,\n",
    "    test_dataset=t_test,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of results with training time and train loss\n",
    "print(f\"\\n{color.BOLD}{'='*100}{color.END}\")\n",
    "print(f\"{color.BOLD}SUMMARY TABLE - CHECKPOINT COMPARISON {color.END}\")\n",
    "print(f\"{color.BOLD}{'='*100}{color.END}\\n\")\n",
    "\n",
    "# Header with training information\n",
    "print(f\"{'Epoch':<10} {'Train':<20} {'Agglomerative':<30} {'KMeans':<30}\")\n",
    "print(f\"{'':10} {'Loss':<10} {'Time(min)':<10} {'Silhouette':<15} {'Purity':<15} {'Silhouette':<15} {'Purity':<15}\")\n",
    "print(f\"{'-'*100}\")\n",
    "\n",
    "for result in checkpoint_results:\n",
    "    epoch_num = result['epoch']\n",
    "    \n",
    "    # Get train loss and time if available\n",
    "    train_loss_str = \"N/A\"\n",
    "    train_time_str = \"N/A\"\n",
    "    if 'CHECKPOINT_EVAL_LOGS' in globals() and epoch_num in CHECKPOINT_EVAL_LOGS:\n",
    "        train_loss_str = f\"{CHECKPOINT_EVAL_LOGS[epoch_num]['train_loss']:.4f}\"\n",
    "        train_time_str = f\"{CHECKPOINT_EVAL_LOGS[epoch_num]['training_time_minutes']:.2f}\"\n",
    "    \n",
    "    print(f\"{epoch_num:<10} \"\n",
    "          f\"{train_loss_str:<10} {train_time_str:<10} \"\n",
    "          f\"{result['agg_silhouette']:<15.4f} {result['agg_purity']:<15.4f} \"\n",
    "          f\"{result['kmeans_silhouette']:<15.4f} {result['kmeans_purity']:<15.4f}\")\n",
    "\n",
    "print(f\"{'-'*100}\\n\")\n",
    "\n",
    "# Identify best checkpoint for each metric\n",
    "best_agg_sil = max(checkpoint_results, key=lambda x: x['agg_silhouette'])\n",
    "best_agg_pur = max(checkpoint_results, key=lambda x: x['agg_purity'])\n",
    "best_km_sil = max(checkpoint_results, key=lambda x: x['kmeans_silhouette'])\n",
    "best_km_pur = max(checkpoint_results, key=lambda x: x['kmeans_purity'])\n",
    "\n",
    "print(f\"{color.GREEN}Best performance:{color.END}\")\n",
    "print(f\"  Agglomerative Silhouette: Epoch {best_agg_sil['epoch']} ({best_agg_sil['agg_silhouette']:.4f})\")\n",
    "print(f\"  Agglomerative Purity:     Epoch {best_agg_pur['epoch']} ({best_agg_pur['agg_purity']:.4f})\")\n",
    "print(f\"  KMeans Silhouette:        Epoch {best_km_sil['epoch']} ({best_km_sil['kmeans_silhouette']:.4f})\")\n",
    "print(f\"  KMeans Purity:            Epoch {best_km_pur['epoch']} ({best_km_pur['kmeans_purity']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafici comparativi delle metriche\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "fig.suptitle('Confronto Performance Checkpoint sul Test Set', fontsize=16, weight='bold')\n",
    "\n",
    "epochs = [r['epoch'] for r in checkpoint_results]\n",
    "\n",
    "# Train Loss (se disponibile)\n",
    "if 'CHECKPOINT_EVAL_LOGS' in globals():\n",
    "    train_losses = [CHECKPOINT_EVAL_LOGS.get(e, {}).get('train_loss', None) for e in epochs]\n",
    "    if any(tl is not None for tl in train_losses):\n",
    "        axes[0, 0].plot(epochs, train_losses, \n",
    "                       marker='*', linewidth=2, markersize=10, color='darkred')\n",
    "        axes[0, 0].set_xlabel('Epoca', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('Train Loss', fontsize=12)\n",
    "        axes[0, 0].set_title('Train Loss per Checkpoint', fontsize=14, weight='bold')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_xticks(epochs)\n",
    "\n",
    "# Training Time (se disponibile)\n",
    "if 'CHECKPOINT_EVAL_LOGS' in globals():\n",
    "    train_times = [CHECKPOINT_EVAL_LOGS.get(e, {}).get('training_time_minutes', None) for e in epochs]\n",
    "    if any(tt is not None for tt in train_times):\n",
    "        axes[0, 1].plot(epochs, train_times, \n",
    "                       marker='o', linewidth=2, markersize=10, color='darkblue')\n",
    "        axes[0, 1].set_xlabel('Epoca', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('Tempo (minuti)', fontsize=12)\n",
    "        axes[0, 1].set_title('Tempo di Training Cumulativo', fontsize=14, weight='bold')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_xticks(epochs)\n",
    "\n",
    "# Agglomerative Silhouette\n",
    "axes[1, 0].plot(epochs, [r['agg_silhouette'] for r in checkpoint_results], \n",
    "                marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[1, 0].set_xlabel('Epoca', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1, 0].set_title('Agglomerative Clustering - Silhouette', fontsize=14, weight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(epochs)\n",
    "\n",
    "# Agglomerative Purity\n",
    "axes[1, 1].plot(epochs, [r['agg_purity'] for r in checkpoint_results], \n",
    "                marker='s', linewidth=2, markersize=8, color='seagreen')\n",
    "axes[1, 1].set_xlabel('Epoca', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Purity Score', fontsize=12)\n",
    "axes[1, 1].set_title('Agglomerative Clustering - Purity', fontsize=14, weight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xticks(epochs)\n",
    "\n",
    "# KMeans Silhouette\n",
    "axes[2, 0].plot(epochs, [r['kmeans_silhouette'] for r in checkpoint_results], \n",
    "                marker='^', linewidth=2, markersize=8, color='coral')\n",
    "axes[2, 0].set_xlabel('Epoca', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[2, 0].set_title('KMeans Clustering - Silhouette', fontsize=14, weight='bold')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "axes[2, 0].set_xticks(epochs)\n",
    "\n",
    "# KMeans Purity\n",
    "axes[2, 1].plot(epochs, [r['kmeans_purity'] for r in checkpoint_results], \n",
    "                marker='D', linewidth=2, markersize=8, color='mediumpurple')\n",
    "axes[2, 1].set_xlabel('Epoca', fontsize=12)\n",
    "axes[2, 1].set_ylabel('Purity Score', fontsize=12)\n",
    "axes[2, 1].set_title('KMeans Clustering - Purity', fontsize=14, weight='bold')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "axes[2, 1].set_xticks(epochs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione t-SNE per tutti i checkpoint confrontati con KMeans\n",
    "num_checkpoints = len(checkpoint_results)\n",
    "fig, axes = plt.subplots(1, num_checkpoints, figsize=(10 * num_checkpoints, 8))\n",
    "\n",
    "if num_checkpoints == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "fig.suptitle('Confronto Clustering KMeans tra Checkpoint (t-SNE Visualization)', \n",
    "             fontsize=16, weight='bold')\n",
    "\n",
    "for idx, result in enumerate(checkpoint_results):\n",
    "    # Applica t-SNE agli embedding\n",
    "    reduction_model = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    vis_embed = reduction_model.fit_transform(result['test_embedding'])\n",
    "    \n",
    "    # Visualizza il clustering\n",
    "    axes[idx].scatter(vis_embed[:, 0], vis_embed[:, 1], \n",
    "                     c=result['kmeans_prediction'], cmap='Dark2', alpha=0.6, s=10)\n",
    "    axes[idx].set_title(f\"Epoca {result['epoch']}\\n\"\n",
    "                       f\"Silhouette: {result['kmeans_silhouette']:.4f} | \"\n",
    "                       f\"Purity: {result['kmeans_purity']:.4f}\",\n",
    "                       fontsize=12, weight='bold')\n",
    "    axes[idx].set_xlabel('t-SNE Dim 1')\n",
    "    axes[idx].set_ylabel('t-SNE Dim 2')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file for reference\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results_summary = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'test_samples': len(t_test),\n",
    "    'checkpoints': []\n",
    "}\n",
    "\n",
    "for result in checkpoint_results:\n",
    "    epoch_num = result['epoch']\n",
    "    checkpoint_entry = {\n",
    "        'epoch': epoch_num,\n",
    "        'agglomerative': {\n",
    "            'silhouette': float(result['agg_silhouette']),\n",
    "            'purity': float(result['agg_purity'])\n",
    "        },\n",
    "        'kmeans': {\n",
    "            'silhouette': float(result['kmeans_silhouette']),\n",
    "            'purity': float(result['kmeans_purity'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add training time and train loss if available\n",
    "    if 'CHECKPOINT_EVAL_LOGS' in globals() and epoch_num in CHECKPOINT_EVAL_LOGS:\n",
    "        checkpoint_entry['training_time_minutes'] = CHECKPOINT_EVAL_LOGS[epoch_num]['training_time_minutes']\n",
    "        checkpoint_entry['train_loss'] = CHECKPOINT_EVAL_LOGS[epoch_num]['train_loss']\n",
    "    \n",
    "    results_summary['checkpoints'].append(checkpoint_entry)\n",
    "\n",
    "# Save to JSON\n",
    "results_file = f\"{CHECKPOINT_PATH}/evaluation_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n{color.GREEN}Results saved to: {results_file}{color.END}\")\n",
    "print(f\"\\n{color.BOLD}Information saved for each checkpoint:{color.END}\")\n",
    "print(f\"  - Clustering metrics (Silhouette and Purity)\")\n",
    "print(f\"  - Training time (minutes)\")\n",
    "print(f\"  - Train Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
